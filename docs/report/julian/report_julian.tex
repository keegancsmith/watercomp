\documentclass[a4paper,11pt]{report}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{color}
\usepackage{epsf}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{cite}
\usepackage{makeidx}
\usepackage{footmisc}
\usepackage{url}
\usepackage{multicol}
\usepackage[pdfborder=0 0 0]{hyperref}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{program}

% \floatstyle{ruled}
\newfloat{eqn}{thb}{lob}
\floatname{eqn}{Equation}

% \floatstyle{ruled}
% \newfloat{program}{thp}{lop}
% \floatname{program}{Program}


\usepackage[T1]{fontenc}
\usepackage[sc]{mathpazo}
\renewcommand{\ttdefault}{cmtt}
% \linespread{1.05}         % Palatino needs more leading (space between lines)


\addtolength{\textwidth}{\oddsidemargin}
\addtolength{\textwidth}{\evensidemargin}
\addtolength{\textwidth}{-1cm}
\addtolength{\textheight}{1.5cm}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}
\setlength{\topmargin}{0in}

\bibliographystyle{apalike2}
\begin{document}

\pagestyle{empty}

\begin{titlepage}
\begin{center}

\textsc{\Large Honours Project Report \\
\ \\
\ \\
\ \\
\ \\
\ \\
}

{\huge \bfseries
Interframe Compression of Water Dominant Molecular Dynamics Simulations 
\\}

\vfill

\large Julian Kenwood
\\
\small{jkenwood@cs.uct.ac.za}

\ \\
\ \\

\begin{tabular}{cc}
  \multicolumn{2}{c}{\large Supervised By:} \\
  \\ 
  \large Patrick Marias & \large James Gain
  \\
  \small patrick@cs.uct.ac.za & \small jgain@cs.uct.ac.za
\end{tabular}

\vfill

\begin{tabular}{|l|l|c|}
  \hline
  & \textbf{Category} & \textbf{Chosen} \\
  \hline
  1 & Software Engineering/System Analysis & 10 \\
  \hline
  2 & Theoretical Analysis & 0 \\
  \hline
  3 & Experiment Design and Execution & 5 \\
  \hline
  4 & System Development and Implementation & 15 \\
  \hline
  5 & Results, Findings and Conclusion & 15 \\
  \hline
  6 & Aim Formulation and Background Work & 10 \\
  \hline
  7 & Quality of Report Writing and Presentation & 10 \\
  \hline
  8 & Adherence to Project Proposal and Quality of Deliverables & 10 \\
  \hline
  9 & Overall General Project Evaluation & 5 \\
  \hline
  \multicolumn{2}{|l|}{\textbf{Total marks}} & 80 \\
  \hline
\end{tabular}

\vfill

\textsc{\Large Department of Computer Science \\
\ \\
University of Cape Town \\
\ \\}

{\large 2009}

\end{center}
\end{titlepage}

\begin{abstract}

Molecular dynamics simulations are files containing atomic and molecular 
data elapsed over time. State-of-the-art in parallel computers are able 
to generate simulations containing up to 10 million atoms and a large
number of frames. These simulations typically hold uncompressed atom
coordinates stored in binary format.

This report's main contribution is the development of a system through
which compression can be performed. Many simulations contain a large quantity
of water molecules which some of our compression schemes attempt to 
exploit. The compressors that are covered in this report use
the temporal properties of the simulation in order to compress files.
These are often called Interframe compressors.

Five Interframe compression schemes are implemented and tested on
several datasets at three different quantisations: $8$-bit, $12$-bit
and $16$-bit levels. The datasets were chosen based on properties such
as size and the coherence of the motion in the simulation.

The test results show that simple delta-based encoding performs well
at all quantisation levels. At $8$-bit quantisation delta encoding
is able to achieve an average of approximately 10\% compression rate
on across all datasets. However, in the coherent datasets, a much 
greater compression rate is achieved. A trend that was picked up on
was that the greater the number of previous frames, the worse the
Interframe compression schemes tend to perform.


\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\chapter{Introduction}

\pagestyle{plain}
\setcounter{page}{1}
Molecular Dynamics is an area of computer science and computational chemistry where computers are used to determine properties of materials and how collections of molecules interact with each other. Often these simulations are the only method that theoretical chemists have for investigation.\cite{Frenkel2001} Laboratory experiments cannot yield sufficient detail of the microscopic properties of materials, but are good at determining the macroscopic properties of materials, for instance colour and temperature changes.

The most popular use for Molecular Dynamics is for \emph{Protein Folding}, which is important for determining the structure of proteins. Protein folding is a process where a strand of amino acids (the protein) uncoil and form a 3D structure. The structure is determined purely by the constituency of the strand. Using this information, one can simulate an initial strand of amino acids to determine what structure is formed. The use of distributed computing in Molecular Dynamics simulations, where multiple computers are used in the simulation, became popular with the release of Folding@Home.\cite{larson2009folding}

\begin{figure}[!h]
 \center
 \includegraphics[width=0.33\textwidth]{resources/Rabies.png}
\caption{A Rabies Virus surrounded by water molecules. The red and white area represent the different atoms of the water molecules, while the blue area is the Rabies Virus.}
\label{introrabmol}
\end{figure}

The simulations that are run typically generate a lot of data. The simulations need to be run at a very small timescale, on the order of $10^{-12}$ seconds and can contain hundreds of thousands of atoms. All of which needs to be saved in order to be studied by an expert user, a chemist or biologist, at a later date. Certain simulations, known as explicit solvent simulations, consist of a large amount of water molecules, or other solvent, surrounding an object of interest. 

These simulations are typically much larger than normal simulations, as each solvent atom needs to be represented. This results in files which can several gigabytes in size. Figure \ref{introrabmol} shows a typical picture of an explicit solvent simulation. The molecule is surrounded by a large amount of water. Roughly, 70\% of the atoms in the simulation belong to water molecules.

For many simulations, compression is particularly important as it can take an exceedingly long time to transmit all the data over slow internet connections. Our system will be specifically designed to tackle the problem of of water molecules in solvent simulations. This is a good problem to tackle, because there is very little research on compressing simulations and no past research on using compressing water solvent simulations.

\section{Research Question}

Our research will attempt to establish which compression scheme is best for molecular simulations that contain water solvent. Our research question that we will attempt to answer is:

\begin{itemize}
 \item Can a scheme which uses knowledge about the atoms' previous positions be used for compression? And if so, which scheme performs best?
\end{itemize}


We will determine which scheme is best by using two criteria: compression rate and compression speed. The compression rate is defined as the ratio between the original file size and the compressed file size. This yields a percentage where a lower number implies high compression was acheived and a higher number means a less effective compression. The speed of a compression will be defined as the time taken to perform the compression measured in seconds.

The key success factor will be determination of which particular scheme performs best. Work from the project could be included in VMD\cite{vmd}, a popular program for visualisation of molecules and simulation, so determining the best scheme is important.
 
\section{System}

To determine these results, we implemented a system that supports experimentation. The system is built-up of two sections: a compression section and a molecular visualisation section. 

The compression section contains all the compressors and the shared components required for each compressor to work. Each compressor can be run via separate drivers. The visualisation component is able to show the output of compression. Each compression scheme uses a process called quantisation where the atoms are irreversibly transformed to make them more amenable to compression. The visualisation facilitates the viewing and analysis of this transformation. 

We plan to tackle the problem of compressing water dominant molecular simulations using two types of schemes: 
\begin{itemize}
 \item \emph{Intraframe} compression: Compression schemes which rely on the spatial and physical properties of the simulations.
 \item \emph{Interframe} compression: Schemes which rely on the temporal, how the simulations evolve in time, properties of the simulations. 
\end{itemize}

The key hypothesis that will be tested by the system is that one or a few compression schemes will perform much better than other schemes. The system can accomplish by performing each compression scheme indepedantly of other parts, such as the visualisation.

\section{Ethics}

There are several ethical issues that are relevant to our project. The first issue is with the quantisation process. Since the process is irreversible, the users must be aware that the compressed file does not necessarily represent the original simulation. The quantisation process, however, is a necessary part of compression. The user is also able to select the level of quantisation in order to balance the trade-off between simulation fidelity and compressed file size. 

We use several libraries in our system. On the compression side we use the Approximate Nearest Neighbours library(ANN) in order to determine information about the water in the simulation. We also use QT, a graphical user interface library, and GTS, a library for surface decimation, for the visualisation process. All of these libraries are released under LPGL. The LGPL is an open source license most commonly found with software libraries. It allows us to use products released under it in non-commerical products. 

The license that we have chosen for our project is the MIT license. This license allows free distribution of our work, but prohibits programmers from using our code in a project released under any other license. This prohibits people from using our work in a commerical project.

\section{Outline}

In Chapter 2 the background necessary for the project is discussed. This is followed by the design of our project in Chapter 3 and the implementation specifics in Chapter 4. Results and Analysis will be presented in Chapter 5. Finally, we conclude with a discussion of the main impact of our project and possible future work in Chapter 6.


\chapter{Background}

There has been a great deal of work in compression over the past two decades\cite{jain1981image}\cite{storer1988data}, however, there is very little research on compressing Molecular Dynamics data specifically.
The following sections will describe the background necessary to an understanding of our approach.
Section \ref{back_mdf} describes the most dominant format that is used for storing the simulations on hard disk,
while Section 2.3 and Section \ref{back_inter} describe various techniques used in the related field of video compression.

\section{Molecular Dynamics Formats}
\label{back_mdf}

There are several Molecular Dynamics Simulation formats. Chief among these formats is the binary \emph{DCD} file. This is the principle format used in Visual Molecular Dynamics(VMD). This format also relies heavily on an existing \emph{PDB} file in order to make sense of the output.

The PDB file stores information about each specific atom in the simulation. This generally includes the atom's element group, the latter is only important for use in protein simulations. The data in the PDB file is stored in ASCII so the file is often larger than necessary. However, the PDB file only stores some initial information for each atom rather than for each frame in the simulation. The DCD format consists of the frames of the simulation containing all the atom's positions for each timestep. For this reason, a large amount of information is redundant when using DCD files in conjunction with PDB files: The PDB file stores positions for each atom, which are ignored when a DCD file is loaded. This means that the PDB files are much smaller than the corresponding DCD files and as succompressing the DCD files is the main priority for our project.  The main reason for the redundant information is to enable the molecules to be viewed in their normal state using just the PDB file and nothing else.

\subsection{DCD format}
The DCD format has various types which all differ slightly from each other. These differences may be ignored for most purposes, as they are normally stored as extra information in the header of the DCD file. VMD is able to handle all major variants such as the CHARMm and X-PLOR formats. However, the most popular format used is the X-PLOR format. \cite{vmddcdformat}

All these types have very slight differences which are mainly for keeping information about the original simulation, for instance the timestep between each frame. The DCD file format does not specify a prefered endian representation. The endianess of the data can be identified by magic numbers stored in the header, but any system that allows for loading DCD files must be able to handle both little and big endian correctly. The header also contains information relating to the number of atoms and the number of frames in the simulation.

Most of the file is dedicated to storing the frames of the simulation. Each frame of the file represents a single timestep in the simulation. In each frame the positional information about each atom is stored. The positions are stored as triples of $x$, $y$ and $z$ 4-byte single precision floating point numbers. This makes up the majority of the file, as each atom repeated in every frame requires 12 bytes to specify fully. The order of the atoms in the frame is important as they correspond to the order of the atoms in the PDB file. No further information is stored in the file after the end of the final frame. The size of the final file can be estimated using the formula: 

\begin{center} $size = 12*frames*atoms$ bytes\end{center}

The above formula is only an estimate as it does not account for the header. Another problem is that in certain format types there can be atoms which remain fixed in space for the duration of simulation. These atoms are only stored in the first frame rather than in all frames. It is reasonable to ignore these effects in most simulations as they typically only account for a fraction of the original atoms. In all the simulations we looked at, none have any fixed atom information. Molecular simulations may have millions of atoms stored at each step and can involve many thousands of frames. This can result in files that are several gigabytes in size and in extreme cases, several terabytes in size.

These simulations need to be archived so that they can be reviewed by a molecular biologist at a later date or they may need to be transmitted over relatively slow connections such as the Internet. Given their large size, it is often necessary to use compression.

\section{Molecular Simulation Compression}
\label{back_msc}
To date, there has been little research on compressing large-scale molecular dynamics simulations. The current method\cite{RefWorks:3} for compression is able to acheive output sizes smaller than the original by an order of magnitude. The method does not have any specific name, so for the remainder of this report we will refer to it as \emph{Omeltchenko's} algorithm after its principal developer. There are two other algorithms of note that are useful for this situation. These are the \emph{Gandoin \& Devilliers} algorithm\cite{devillers2000gci} and the \emph{Predictive Point Cloud Compression} algorithm\cite{gumholdcomp}. Both of which are used for compressing point cloud data. A point cloud is a collection of points, where each point has position and possibly other attributes. This is of relevance because atoms stored in a PDB file may be treated as a point cloud.

\subsection{Omeltchenko's Algorithm}
\label{bgomel}

The algorithm begins by quantising the cuboid that encapsulates the simulation, also known as the bounding box. This space is usually taken to be the axis-aligned bounding box that encompasses all the atoms, and this has to be recalculated on a per-frame basis. A three dimensional grid of a particular granularity is embedded into the space. Atoms are snapped to the centers of the closest grid cell. This transformation introduces a small error, which can be controlled by the granularity of the grid, but enables the algorithm to reduce the floating point coordinates to integer-based grid cell indices. This step, known as quantisation, is commonplace in point compression. It allows insignificant variations in the data to be removed, while introducing a quality setting to allow for better compression at the cost of accuracy. This stage produces a quantised $(x, y, z)$ coordinate for each atom. As a post operation, the points are translated so that all coordinates are non-negative. This process is made reversible by storing the bounding box of the original atoms.

The atoms' indices are mapped to \emph{Octree Indices}. An \emph{Octree} is a spatial partitioning data structure\cite{wilhelms1992octrees}, which recursively splits the bounding box up into 8 smaller cuboid cells. The subdividing process typically stops when there are few points in a cell or a maximum depth has been reached. A variant of the Octree is the \emph{Linear Octree}\cite{chuang1995efficient}, where the data structure is represented in a 1D array. Instead of using pointers in a tree data structure, the Linear Octree uses arithmetic in an array to calculate each node's index in an array. This usually uses more memory than a normal Octree as it requires the tree to be complete, however we are not interested in constructing the Linear Octree, only calculating the indices at which atoms would be found. Octree Indexing is a reversable spatial hashing algorithm where 3D points are mapped to 1D indices, by using the Linear Octree cells. In this case, the quantised points are mapped to single non-negative integers. The algorithm uses Octree Indexing to interleave the bits of the coordinates of each point. The index is cleverly assembled so that points are only interleaved up to the largest most significant bit of every coordinate. Figure \ref{octree} shows the process for a two atom system.

\begin{figure}[!h]
 \center
 \includegraphics[width=0.6\textwidth]{resources/OctreeIndexingVerticalCropped.png}
\caption{Demonstration of the Octree Indexing Process on a system of 2 atoms. The first step of the process is conversion to binary. From this we can see that the $x$ components require 4-bits to encode as an index, while the $y$ and $z$ components require 3-bits. The final step of the prcoess is the interleaving of the bits in each atom's coordinates to produce a final number.}
\label{octree}
\end{figure}

The resulting indices are stored in a list and sorted in increasing order. These indices are written to the file using a \emph{delta encoding} scheme. The first atom's position indicates its actual grid index, while the rest are stored as differences between the previous index and the current one. This final transformation typically allows a better compression rate to be acheived. Delta encoding reduces the amount of data that needs to be compressed by keeping things small and making it more amenable for compression, however it introduces a dependancy on decoding all the previous atoms in the list first. The output, itself, is performed by an adaptive, variable-length compressor. Typically, each index can be stored in a single 4-byte integer which results in an immediate size reduction from 12-bytes per atom to 4-bytes per atom. 

The compressor encodes the molecular data as a series of non-negative integers. Each integer streamed into the compressor one at a time. The compressor maintains an integer $l$ which is the current size at which integers are encoded as. As each integer comes in, it is checked to see if the integer can be stored using $l$ bits. If it can, it is stored as $l+1$ bits; a 0-bit followed by the bits of the integer. If it cannot, the integer is partly stored as $l+1$ bits; a 1-bit followed the first $l$ bits of the integer. The rest of the bits are compressed in the next step of the compressor. If enough of these overflows occur, $l$ is incremented.

The compressor was designed to be computationally undemanding as computers at the time of the paper were not as powerful as current machines. We found that it is not a good compression system as it is not based on any of the established variable-length encoders, which have been shown to produce near-optimal encodings.

The paper does not use the common file formats such as those used in VMD, it does, however, treat atoms very similarly to the format used in DCD files, so we are able to use the scheme for compression. The algorithm deals with atoms that are represented by a block of 56-bytes, containing position (24-bytes), velocity (24-bytes) and an ID (8-bytes). The paper claims that the 56-byte atomic data is encoded at approximately 6.3 bytes per atom. In our application, however, the atoms are defined only as 12-byte position data, but have an implicit ordering from the PDB file. Since the sorting destroys the ordering, additional information will need to stored to recover the initial ordering when decompressing.

\subsection{Gandoin \& Devillier's Algorithm}
\label{gdkdtree}
The Gandoin \& Devillier's Compression Algorithm\cite{devillers2000gci} exploits a property of \emph{KD trees}\cite{moore1991intoductory} to compress Point Cloud Data. A KD tree is a spatial subdivision data structure which partitions space using various axis-aligned splitting planes. The KD tree cycles through each dimension for the splitting axis as the data structure is recursively constructed. The splitting planes are chosen to divide the space in half at each step. This differs from Octrees in the way that space is divided: Octrees split space into four subdivisions of equal area, while KD trees only split space into two and do not require that the areas are equal.

Gandoin and Devillier's algorithm exploits the property that if one knows the number of points over the entire space and the number of points in one of the regions created from the split, one can work out the number of points for the other region by simple arithmetic.

\begin{figure}[!h]
 \center
 \includegraphics[width=0.5\textwidth]{resources/GaD.png}
\caption{A demonstration of the Gandoin \& Devillier encoding process. 7 points are being compressed. Numbers in brackets are the calculated values, while the others are encoded. \cite{devillers2000gci}}
\label{octree}
\end{figure}

Each leaf node of the tree is subdivided if there are multiple points in the region. The points in the compression are usually quantised. The original frame may only have distinct points, however, the quantised frame could have several duplicates depending on the quality of the quantisation. For this reason, the subdivision might stop before the number of points in a cell is less than 2.

An advantage of this algorithm is that it is progressive. This means that low-detail representations of the point cloud can be constructed from the initial parts of the compressed stream. As the compression progresses the detail in the model becomes finer until the output reaches the final quantised result. This algorithm produces a symbol stream which is usually encoded with an entropy encoder such as \emph{Arithmetic Coding}\cite{RefWorks:4}.

\subsection{Predictive Point Cloud Compression Algorithm}

Predictive Point Cloud Cloud Compression is a new approach to compressing point cloud data. The compression scheme creates a spanning tree of the original point set using heursitics.\cite{gumholdcomp} Predictors based on these heuristics exploit patterns in the data to get an estimate of the position of a child in the tree relative to its parent. The use of these predictors means that only the residual error from each prediction needs to be stored.

In the original paper the algorithm only used one of two predictors for the models to encode the entire pointset. The user was prompted for which predictor to use. The first predictor predicts that the child would be placed in the same position as the parent, while the second predictor estimates the child to have the same displacement as the displacement between its grandparent and parent. Further enhancements to the algorithm were made to choose the best predictor at each step of the algorithm and also extra predictors were added.\cite{merrycomp}

The performance of this algorithm depends greatly on the quality of the predictors used. Poor predictors will result in significantly worse compression. The compression scheme requires the number of small errors to be high, and performance decays very rapidly as the the error size increases. The predictors also require that the point cloud represent a surface so that information such as the surface normal at each point can be estimated. Our data does not lie on a surface so some adaptation of the predictors is necessary.

\section{Video Compression}

A different approach to follow is that of video compression. Essentially, the molecular simulation can be viewed as a video, but using 3D molecular data instead of 2D picture data for each frame. We can safely ignore the audio part of compression as it has no relevance to molecular simulations. Video Compression loosely falls into one of two subcategories. The first is \emph{intraframe compression} which seeks to compress each individual frame using little or no information about previous frames. The other category is \emph{interframe prediction} which uses a significant amount of information from previous frames to compress a frame. 

Usually, each frame is heavily analysed for objects that are considered important. These objects are often major features such as a person's arm or a ball. The purpose of the intraframe compression is to record these objects with their initial positions, while interframe prediction estimates the position of the objects and only records the errors that occur from making the estimate. In most cases, given a good predictor, the error is usually small and can be compressed very well.

The rest of this section will discuss the algorithms used in both intraframe compression and interframe prediction. The techniques used to detect objects in the images are ignored, as our simulations have atoms which are already defined objects.

\subsection{Intraframe Compression}
\label{back_intra}

The latest MPEG encoding algorithms allow for a choice of several compression algorithms to be used\cite{gall1991mvc}. This section will focus on the most common algorithms used for compressing the objects in the original image. These algorithms can be classified as either \emph{lossless} or \emph{lossy}. Lossless compression algorithms are able to recreate the original data with no loss of information. These lossless compression algorithms are general enough to apply to standard data compression. Lossy compression algorithms, on the other hand, are able to achieve much higher compression rates, but do incur the penalty of some information loss. Typically, the lossy compression algorithms first quantise the input to fit within a certain number of symbols and still require further compression from a lossless algorithm. We will focus mainly on the lossless compression techniques as information loss, aside from small quantisation errors, may result in incorrect assumptions being drawn from simulations.  

These techniques fall into two categories: Entropy encoders and dictionary encoders. 

\subsubsection{Entropy Encoders}

Entropy encoding makes use of Information Theory and tries to build an optimal mapping from symbols in the frame to individual bits. In a video frame, this information could be lossily compressed blocks of pixel data or information about identified object. In a molecular simulation we could easily use this to store positions of molecular data.

The first algorithm of this type developed was \emph{Huffman Encoding}\cite{citeulike:1320251}. The original Huffman Encoding relies on having frequencies of all symbols that occur in the message. It uses this table to build up a prefix-free tree. A prefix-free tree's leaves are symbols and the paths from the root to a leaf indicates the bits required to output that symbol. The frequencies allow the algorithm to construct an `optimal` tree that requires less bits to output common symbols and more bits to output rare symbols. Decoding a single symbol from a message  entails following the paths indicated by the bits until a symbol is reached, or looking up the bit patterns in a table or codebook. Figure \ref{huffman} shows this process for a sample alphabet.

\begin{figure}[!h]
 \center
 \includegraphics[width=0.4\textwidth]{resources/HuffmanTreeCropped.png}
\caption{A Huffman Tree for a simple alphabet of \{A,B,C,D,E\} with frequencies \{2, 3, 4, 5, 5\}. A and B have a comparatively low frequency to the C, D and E and thus require more bits to encode. The codebook constructed from this tree would be \{A: 000, B: 001, C: 01, D: 10, E: 11 \}}
\label{huffman}
\end{figure}

There are various issues when using regular Huffman Encoding. The first problem is that the entire message must be available when writing the compressed data in order to compute the frequency table.\cite{RefWorks:1} The second problem is that in order to decompress we need to also output the entire prefix-tree used for compression. These problems can be remedied by \emph{Adaptive Huffman Encoding}\cite{42227} which is able to re-optimise the prefix-tree while the compression occurs.

The Adaptive Huffman Encoding scheme is a good candidate for compression of the molecular simulation frames. The memory footprint of the algorithm is low: only the prefix-free tree needs to be stored while the algorithm runs and has a worst-case space usage of $O(S)$, where $S$ is the number of unique symbols in the frame. The algorithm can also be run while the simulation takes place as only a small amount of computation needs to occur per atom.

There is a similar entropy-based compression algorithm called \emph{Arithmetic Coding}. This algorithm has an advantage over Huffman Encoding as it can store symbols as a fractional number of bits. \cite{RefWorks:1}, \cite{RefWorks:3}. The amount of wasted bits that appear in Huffman Encoding can be as bad as 1 bit per symbol. Arithmetic Coding works by encoding the entire message into a single rational number in the range [0, 1). The frequency of each symbol is used to allocate large subranges to frequent symbols and small subranges to rare symbols. Figure \ref{arithmeticcoder} shows the arithmetic coder ranges for the same alphabet as in \ref{huffman}.

\begin{figure}[!h]
 \center
 \includegraphics[width=0.8\textwidth]{resources/ArithmeticCoder.png}
\caption{Arithmetic Coder ranges for same alphabet in Figure \ref{huffman}}
\label{arithmeticcoder}
\end{figure}

Encoding a specific symbol simply requires adjusting upper and lower bounds of the output number. When the entire message is encoded the number can be output. Decoding requires one to follow the subranges that the encoded message falls into. Figure \ref{arithmeticcodersubrange} shows the effects of encoding or decoding on the ranges of the arithmetic coder.

\begin{figure}[!h]
 \center
 \includegraphics[width=0.8\textwidth]{resources/ArithmeticCoderSubrange.png}
\caption{Arithmetic Coder ranges for the Arithmetic Coder in Figure \ref{arithmeticcoder} after encoding a D}
\label{arithmeticcodersubrange}
\end{figure}

Arithmetic Coding implementations are typically very complex. Some implementations use a static dictionary of 256 symbols, while others are able to handle a dynamic dictionary of up to a few thousand symbols. The reason for this is that the numbers for the ranges are not implemented using an arbitrary precision rational number library. Instead they use a representation based on regular 32-bit integers. Overflow and underflow issues are handled by scaling the range.

Unfortunately, there are problems with the original Arithmetic Coding algorithm in that you need to compute the entire frequency table so that it can be output before compression begins. The \emph{Adaptive Arithmetic Coding} algorithm is an adjustment that does not require the frequency table to be constructed beforehand. It does, however, require that symbols are stored prior to any output. 

Both the Adaptive Arithmetic Coding algorithm and the Huffman Coding algorithm require a separate component for handling the frequency table. Unlike, the regular static algorithms, this frequency table needs to allow insertion of new symbols, updating the frequencies of existing symbols as well as determining which symbol a given frequency belongs to and what the frequency range of a symbol is. Two common data structures for storing this information are arrays and Fenwick Trees\cite{fenwick1994new}.

In most implementations of Arithmetic Coding, the encoding algorithm is separated from the static or adaptive parts of the algorithm. The core algorithm is usually called the Encoder and Decoder, as it is only responsible for encoding and decoding ranges. The encoding and decoding part is independant of whether one uses the adaptive or static algorithm. The parts of the algorithm that handle the adaptive and static part is called the \emph{Model}. This separation allows for better modularity and design of an arithmetic coder.

The Arithmetic Encoding scheme offers a better choice for compression of the frames. The adaptive models allow us to update symbol frequencies while the frame is being compressed. This leads to the same worst-case space usage as Huffman Encoding. The adaptive algorithm also requires only a small amount of computation per symbol. However, the problem of conveniently storing the symbols for each output file remains.

\subsubsection{Dictionary Encoders}

Dictionary-based encoding schemes attempt to compress data using a different approach from entropy encoders\cite{RefWorks:2}. A data structure known as a \emph{dictionary} is able to efficiently keep track of a sliding window of text information. This window allows different dictionary encoding algorithms to take advantage of distributions of symbol placement data in the input message. Rather than using frequencies as a basis for shorter codewords, they resort to greedily assigning symbols or groups of symbols to codewords of increasing length. This gives certain advantages over entropy encoders, such as speed and requiring little or no mapping information necessary to decode the data. 

There are various dictionary-based compression algorithms, key among them are the \emph{LZW} and \emph{DEFLATE} algorithms. The Lempel-Ziv-Welch(LZW) algorithm keeps an explicit dictionary that is progressively built up as the stream is read.\cite{1320134} Using this information, a codeword for the current symbol can be worked out and written to file. This algorithm needs to store additional information with the text in order to reconsitute the initial dictionary, but after this the message can be recreated by dynamically updating the dictionary. 

The DEFLATE algorithm uses several techniques such as an additional layer of Huffman Coding for bit-reduction, a sliding window based dictionary and a duplicate string removal algorithm\cite{deflaterfc}. It is the most commonly used dictionary-based compression algorithm and is used in applications such as gzip and in the PNG image format. 

Dictionary-based encoding schemes tend to perform worse on average than entropy-based encoders, but can perform much better for certain messages, for instance, where there are large runs of repeated information. Dictionary-based encoding schemes require very little computation per symbol and have low memory overheads. However, these overheads can be larger than in entropy encoders. 

\section{Interframe Prediction} 
\label{back_inter}

Interframe prediction takes advantage of the correlation between the current frame and previous frames. Objects that are identified are predicted to be in a new position. Most of the time, the prediction will be incorrect, but, if the predictors are chosen carefully, the predicted position will be close to the actual position. If the errors are small, they can be effectively compressed, using an entropy encoder, as individual errors are likely to occur more frequently. There are several types of prediction that are in common use. We will discuss, specifically, \emph{first-order prediction}, \emph{K-th order prediction} and \emph{Kalman Filtering}.

There are several noticable differences with the prediction schemes suitable for standard video formats and the molecular dynamics simulations. The first difference derives from the fact that these schemes require some information to bootstrap the prediction. Frames in molecular dynamics simulations can use positions of the atoms in previous frames to predict on, while videos often only have pixels. The latest compression schemes break the video up into blocks. Objects are identified within these blocks. Prediction can the be performed on the objects giving a high rate of compression. The MPEG-4 format specifies a single prediction scheme with a large number of features\cite{wiegand2003oha}. 

A related topic is that of index frames. One of the reasons this prediction is not used completely throughout a video file is that it makes every frame in the file dependant on all the frames that precede it. In order to decode a frame in the file, every prior frame must be decoded. To allow for random access in a video, certain frames are labelled as Index frames or I-frames which are compressed only using Intraframe compression. Typically, a structure called a Group Of Pictures will be given that defines the order of the I-frames, and predicted frames, or P-frames\cite{vandalore2001sal}. There are also frames that can predicted from the previous or the following frames, called B-frames. This structure is useful to have in any compression of molecular dynamics simulations since it allows for random access in the simulation using both intraframe and interframe compression.

\subsubsection{First Order Predictors}

First order predictors use the current velocity of an object as a predictor of where the object will appear in the next frame. The velocity is calculated as the difference between the positions of the object in the previous two frames. This scheme would be particularly easy to implement in a molecular dynamics simulation where objects are explicitly provided.

The \emph{error} is coded as the difference between the predicted and actual position of the object. Given the physical nature of the simulations, this error is likely to be small, however, in a normal video there can be situations where this predictor does not perform very well. Figure \ref{linearencoding} demonstrates the process of using the velocity of the atoms to predict subsequent positions.

\begin{figure}[h]
 \center
 \includegraphics[width=0.8\textwidth]{resources/FirstOrderEncoding.png}
\caption{Encoding of a point using First Order encoding. The atom is moving from the left to the right. Using the previous two frames, the new position is guessed using an estimated velocity. This guess is not necessarily accurate, so error terms for each dimension are also encoded.}
\label{linearencoding}
\end{figure}

\subsubsection{K-th Order Predictors}

K-th order predictors are an enhancement of the first order predictor. Instead of just the positional data of the last two frames, they use an object's positional data over the last $K+1$ frames to calculate the first $K$ derivatives of the object's position. Essentially, a curve is calculated that passes through all $K+1$ previous positions. These predictors are updated when the next frame is encoded and used to predict the next position of the object.

\begin{figure}[h]
 \center
 \includegraphics[width=0.6\textwidth]{resources/DeltaEncoding.png}
\caption{Encoding of a point using the 0th Order encoding. The atom moves a fixed amount per frame. The errors are the amounts moved from the previous time for each dimension.}
\label{deltaencoding}
\end{figure}

The error is again coded as the difference between the predicted and actual position of the object. However, there are several drawbacks to using K-th order predictors. The positional data of each of the objects must be kept from the previous $K+1$ frames. Also, each prediction requires $O(K^2)$ work to be performed per object. Another problem is that as $K$ increases, the quality of the predictions does not necessarily increase, and could even decrease, resulting in large error rates. If the points are not moving in a coherent fashion, that is their motion is not unpredictable, then this prediction will not do well.

Figure \ref{deltaencoding} shows the example algorithm where K = 0.
 
\subsubsection{Kalman Filtering}

Kalman Filters are a relatively advanced prediction system that represent the motion of an object as a stochastic process.\cite{welch1995ikf} The predicted position at timestep $k$, $p_k$, of the object can be simulated as a linear stochastic difference equation of the form:
\begin{center} $p_k = Ap_{k-1} + Bu_{k-1} + w_{k-1}$  \end{center}
The actual measurement data, $x$, is related to the current predicted state by:
\begin{center} $x_k = Hp_k + v_k$ \end{center}
$w_k$ and $v_k$ are random variables which are normally distributed and are related to the noise that occurs within the stochastic process. $A$ is a matrix relating the previous prediction to the current prediction. $B$ is a matrix capturing optional control input, for instance additional prediction information. $H$ is a time-dependant matrix which relates the current prediction to the current actual measurement. The Kalman Filtering process adjusts the matrices within the equations to get better and better approximations of the motion of the object.

Kalman filters provides increasingly better predictions as more frames are encoded. The drawback is that several matrix multiplications are required to acheive this accuracy, which can be computationally expensive to do on a per-object basis. This is especially true in large scale molecular dynamics simulations where there could be millions of atoms. This extra computation, however, could have a dramatic impact on the rate of interframe compression due to the very small error rates.

\section{Summary}

There are two main techniques for compressing videos. Intraframe techniques reduce each frame to a collection of objects, in molecular simulations these are the atoms, which are encoded using either entropy encoders or dictionary encoders. Entropy encoders include the techniques of Huffman Coding and Arithmetic Coding. Huffman Coding uses an algorithm to construct a prefix-free tree to map symbols to bits, but can waste up to $1$ bit per symbol. Arithmetic Coding is a more complicated technique which uses rational numbers to encode symbols. Arithmetic Coding can encode symbols as a fractional number of bits, avoiding the wastage that exists in Huffman Coding.

Interframe techniques use temporal coherence to better encode each frame, however this introduces a dependence on some of the previous frames. Interframe compression attempt to guess the next state of objects and encode any errors. If the predictions are accurate, a better compression rate can be acheived. There were two main techniques that were investigated. First order and K-th order prediction compute curves that fit data over a selected window of frames and extrapolate from this curve. Kalman filters treat the motion as a stochastical process that can be estimated with some accuracy through the use of matrices.

\chapter{Design}

The project's main aim was to investigate how effective certain methods of compression would be on molecular simulations. The key success factors of the . This was thus designed in order to test the effectiveness of the compression schemes. Using our system we measure the performance of the compression methods, in terms of speed and size of the compressed file. The actual testing of the system is accomplished via scripting and not directly through our system.

In the remainder of this chapter we will discuss some of the decisions that affected our design. We will follow with a system breakdown and discuss the design of file format used in our compressed files. Finally, there will be a discussion of the experiment design. 

\section{Design Considerations}

We settled for an iterative design methodology. The system was designed so that the first iteration would have very little functionality, but still be able to compress simulations. 

There were three iterations in our design phase:
\begin{enumerate}
 \item Reference compression schemes implemented.
 \item Most compression schemes implemented.
 \item All compression schemes and visualisation implemented.
\end{enumerate}

The initial design of the program consisted of an integrated system that could handle compression, decompression and visualisation of the molecular simulation data. This was changed once we learnt that the compression would possibly be included in VMD. VMD allows the simulations to be visualised using several different representations and so our visualisation would not be useful for VMD.

The program was then split-up into components that could be shared across multiple programs, namely the visualisation and the compressors. This was necessary as there was functionality that needed to be in both the visualisations and each compressor, such as quantising and dequantising points. The graphical front-end of the program would be handled entirely in the Visualisation section, while the compression back-end would be split-up between the Intraframe section and Interframe section. 

Another motivation for splitting up the work load is that testing requires each part to be separated in order to more efficiently gather results. The final design allowed the compressors to run independently with a separate driver program for each one.

Another important design choice was to separate the Interframe and the Intraframe compression schemes. In normal video compression schemes they are combined into a single scheme. In order to adequetely measure the performance of the interframe compression schemes it is necessary decouple them. The use of other schemes would make it difficult to measure the compression time and compression rate. 
 
The system is designed to be very memory efficient. As little memory would be used as possible in order to carry out the compression. Our main reason for this was that the data we were dealing with was very large. Individual frames were potentially several gigabytes in size. The current state of the art in simulations has a much lower size limit of several megabytes\cite{JSconv}. The system that is implemented is less stringent on memory which simplifies it greatly.

The final requirements for the compression section of our system are that it would needs to be able compress large simulations with large numbers of atoms and frames. We define large simulations as consisting of at least:

\begin{itemize}
 \item $2,000,000$ atoms
 \item $10,000$ frames
\end{itemize}

From these requirements, we calculated that the size of each frame is about $20MB$ at maximum. The intraframe compression schemes only hold each frame once as they are being compressed. However, the interframe compression schemes require a window of previous frames. Typically, this window is very small, so the impact of storing each frame in the window is negligble.

It is predicted that in the next 2 years, the state of the art clusters will be to create simulations containing between $10,000,000$ and $100,000,000$ atoms. In order to handle these increases our system may need to be optimised in order to allocate as little memory as possible.


\section{System Overview}

\begin{figure}[h]
 \center
 \includegraphics[width=0.7\textwidth]{resources/Breakdown-connect.pdf}
\caption{System breakdown}
\label{sysbreak}
\end{figure}

The system is divided up into several sections (Figure \ref{sysbreak}): Visualisation, Compressors and Shared Components. The Visualisation handles viewing of the output of molecular simulations that have been compressed by our system. It allows users to see the effects of quantisation on the simulations. This section's design and implementation is described in the Visualisation thesis\cite{minvis} for this project. 

The Compressors section contains all the compression techniques that were implemented. The compressors fall into one of two subsections: Intraframe and Interframe compression. Most of the Intraframe compressors are explained in the Intraframe thesis\cite{kegcomp}, however, the design and implementation of the Omeltchenko Encoder will be explained later in this thesis. (Section \ref{imp_omelt})

The Shared Components part of the system represents various subsystems that are shared among various parts of the project. The main reason for separating these components out is to avoid code duplication. There are additional benefits such as easier testing and better subdivision of the work load among our team.

The following is a list of the component sections that are described in this thesis.
\begin{itemize}
 \item Intraframe Compressors
 \item Interframe Compressors
 \item Simulation I/O
 \item Simulation Processing
 \item Verifiers
 \item Encoder
\end{itemize}

The design of these sections and the components within them will now be described.

\subsection{Intraframe Compressors}

Only one Intraframe Compressor's design will be discussed in this paper, the remaining compressors are described in the Intraframe Compression thesis. 

\subsubsection{Omeltchenko Compressor}

The reference Omeltchenko compression scheme is based on the algorithm outlined in Section \ref{bgomel}. The design is the same as the compression scheme described in the Background Chapter.

\subsection{Interframe Compressors}

There are five Interframe Compressors that were implemented: Polynomial Extrapolation, Spline Extrapolation, Smallest Error Encoding, Common Error Encoding and k-Nearest Neighbour Encoding. The following sections will describe their workings motivate why they were chosen.

\subsubsection{Polynomial Extrapolation}

The Polynomial Extrapolation Compressor is what is used for the K-th order predictor. Instead of using deriviatives, we use polynomial interpolation to construct a function, $F$, that passes through each position. In the end, these two approaches yield exactly the same result, but it is much easier to implement.  

The first data point is treated as lying at position $(0, x_0)$, while the last data point will lie at position $(K-1, x_{K-1})$. Here, $K$ represents the size of the window.  The compressor then predicts that the next position will lie at $F(K)$. The degree of the constructed polynomial is $K-1$. The extrapolated value from this curve is used as an estimate of where the next data point lies. Figure \ref{PolyDescrip} shows an example polynomial created from the a dataset.

\begin{figure}
 \center
 \includegraphics[width=0.75\textwidth]{resources/PolynomialInterpolation.png}
\caption{A polynomial of degree 5 being interpolated through 6 points. }
\label{PolyDescrip}
\end{figure}

\subsubsection{Spline Extrapolation}

The Spline Extrapolation Compressor uses a similar technique to the Polynomial Extrapolation Compressor. The main difference is that instead of forcing a polynomial to pass through each point, it only maintains that the polynomial must go through the first and last points. The remaining points are control points which can be considered as attractors of the curve. These polynomials are called \emph{splines} and can be evaluated just like a regular polynomial.

The main reason for Spline-based prediction scheme is that that the polynomials from the Polynomial Extrapolation scheme tend to oscillate as the degree of the polynomial increases. This oscillation can have detrimental affects on the prediction values. Splines, on the other hand, do not oscillate as much as the because they tend to be very smooth curves. Figure \ref{SplineDescrip} shows an example spline created from a dataset.

\begin{figure}
 \center
 \includegraphics[width=0.75\textwidth]{resources/SplineInterpolation.png}
\caption{A spline of degree 5 being interpolated through 6 points. }
\label{SplineDescrip}
\end{figure}

\subsubsection{Smallest Error Encoding}

Smallest Error Encoding uses a different technique to the previous Compression schemes. Instead of extrapolating based on some function, it uses the previous frames as a 'memory bank'. The compression scheme looks in the memory bank for the closest data point in the window. Just encoding the error from the nearest point is not sufficient as it does not allow reconstruction of the point.

Another piece of information, the index of that nearest point, must also be encoded. This scheme represents a trade-off between keeping the errors from the predictions as small as possible, and hopefully very frequent, and encoding as few symbols as possible. This scheme could, however, encode more symbols than necessary due to the additional index symbols that must be output. Figure \ref{NearestDescrip} shows an example encoding of a point using this algorithm.

\begin{figure}
 \center
 \includegraphics[width=0.75\textwidth]{resources/Nearest.png}
\caption{Example encoding of via Smallest Error Encoding: The point to be encoded is 3. The nearest point, 2, in the prediction window occurs at index 4. The error that is encoded is 1.}
\label{NearestDescrip}
\end{figure}

\subsubsection{Common Error Encoding}

Common Error Encoding is a variation of the Smallest Error Encoding scheme. The scheme still outputs two quantities, and error and an index, but it does not choose errors based on how small they are. It uses knowledge of how many times errors were encoded to make potentially better descisions. Arithmetic Coding is able to compress much better if very few symbols occur very often. The compressor takes advantage of this by keeping track of two sets of information: The number of times a specific error has been encoded and the number of times a specific index has been encoded. 

In order to determine what to encode, the compressor attempts to guess the impact of encoding each error and index in the prediction window. We made the assumption that the Arithmetic Coder will compress well if the probability of encoding an error and a symbol occurring is high. The algorithm then outputs the symbols that occur with highest probability. This probability of a symbol occuring can be calculated as the number of times it has occured divided by the total number of symbols encoded. The probability of an error index and symbol index occuring is equal to the product of their individual probablities, we assume they are independent events.

Figure \ref{CommonDescrip} shows an example of the encoding scheme. The figure only demonstrates the process with errors and not errors and indices for simplicity.

\begin{figure}
 \center
 \includegraphics[width=0.5\textwidth]{resources/Common.png}
\caption{Example encoding of via Common Error Encoding: The point to be encoded is 3. Although, the closest point in the prediction window is 2, it has only been encoded once before. The scheme makes the assumption that encoding an error of 3 is best as it has occured 5 times before.}
\label{CommonDescrip}
\end{figure}

\subsubsection{k-Nearest Neighbour Encoding}

The final scheme implemented is based loosely on a machine learning technique called k-Nearest Neighbours\cite{duda2001pattern}. The scheme works using object classification. Objects are classified by embedding them in a 'feature space.' This space has a dimension for every feature than can distinguish different objects. In this case, we break up the prediction window into contiguous, overlapping vectors of size $V$. 

These vectors make up the objects in our feature space and represent a snapshot of the path in the history of the point. We can use these snapshots to predict where the point should appear next. A similarity metric, such as the Euclidean or Mahalanobis metrics, is used in order to determine the similarity between the current feature vector to previous features vectors in the prediction window. Finally, the error from the most likely position then encoded.

The vectors are assigned an expected value. This expected value is set to the value directly after the vector in the prediction window, or the place where the point is expected to be. Figure \ref{kNNDescrip} shows an example encoding with the scheme.

\begin{figure}
 \center
 \includegraphics[width=0.5\textwidth]{resources/NN.png}
\caption{Example encoding of via k-Nearest Neighbour Encoding: In the top left, the prediction window of \{1, 2, 3, 2, 5, 7\} has been broken up into vectors. In the top right, the latest feature vector, representing the current snapshot in the history of the point. The current point to be encoded is 6. The most similar vector in the prediction window to the current one is last one in the window. The expected value is 7, so an error of -1 is encoded as 7 - 1 = 6}
\label{kNNDescrip}
\end{figure}

\subsection{Simulation I/O}

We chose to use the PDB and DCD file types exclusively as our target formats, since these are the main formats used in VMD. We had separate components for each I/O function we required from our project:

\begin{description}
 \item DCD I/O: This component performs writing and read of the simulation files, but does not handle I/O of compressed data. One main pre-requisite for this components is the ability to skip to a certain frame for visualisation purposes.
 \item PDB I/O: This component handles I/O of the PDB file which contains information about the atoms. It also stores them in a format that is better to work with.
 \item Frame: This component stores non-quantised points that have been read in from a simulation file, or have been de-quantised from a compressed file.
 \item Quantiser: This component stores quantised points data. It contains methods for converting to and from the Frame object.
\end{description}

\subsection{Simulation Processing}

The most significant task that is necessary for visualisation and the Intraframe compression schemes is the grouping of the water molecules in the file into water clusters. A water cluster is simply a collection of water molecules that are bonded together by the attractive force between the oxygen and hydrogen atoms of adjacent water molecules. The visualiser requires all the water clusters in order for certain visualisation modes to work. The Predictive Point Cloud Compressor requires the water clusters to use heuristic predictors. 

The task of extracting water clusters was broken in two parts: The Frame Splitter and the Cluster Extractor. Atoms within the simulation must first be identified based on what molecule they belong to. The most important group is the water molecule category. The water molecules from the Frame Splitter are sent to the Cluster Extractor. The Cluster Extractor's job is to connect the individual water molecules into water cluster groups using a graph data structure.

\subsection{Frame Splitter}

The Frame Splitting is used to break the atoms in a frame into two groups: Water molecules and non-water atoms. This is necessary for both visualisation and compression. The visualisation needs to know which atoms are water consitituents and which atoms are part of other molecules so it can apply different visualisation algorithms to both. On the compression side we need to know which atoms form water molecules for the Predictive Point Cloud Compressor so it apply the heuristics that exploit the characteristics of water molecules.

\subsection{Cluster Extractor}

The cluster extraction component's main job is to join water molecule into a clusters according to a heuristic. The heuristic used is that oxygen atom and hydrogen atoms of adjacent water molecules will line up. The distance between  Using the information from the Frame Splitter, the cluster extracter uses the polar nature of the atoms to predict where other water molecules in the same clusters are. Water molecules are connected if they are close to the predicted position. Figure \ref{WCDescrip} shows an example of water clusters connecting the water molecules in 2D.

\begin{figure}
 \center
 \includegraphics[width=0.5\textwidth]{resources/WaterClusters.png}
\caption{Water clusters connecting water molecules. The dotted lines represent bonds between water molecules. The top two water molecules and bottom-right water molecules is in a single water cluster. The bottom-right water molecule connects to no other water molecules and is in a water cluster with only itself. }
\label{WCDescrip}
\end{figure}

\subsection{Verifiers}

The verifier is a component that can be run on simulations to detect if they have been probably decompressed and dequantised to the correct thing. It also collects statistics on the quantisation process.
We identified 4 statistics to collect:
\begin{itemize}
 \item Minimum Error: 
 \begin{itemize}
   \item The smallest difference between the original and quantised data. This should be as small possible.
 \end{itemize}
 \item Maximum Error:
 \begin{itemize}
   \item The largest difference between the original and quantised data. If this value is too large, then it is can indicate that an error has occured during compression or decompression.
 \end{itemize}
 \item Mean Error:
 \begin{itemize}
   \item The average difference between the original and quantised data. Again, large values can indicate problems in compression or decompression.
 \end{itemize}
 \item Error Variance:
 \begin{itemize}
   \item The variance of the errors statistics between the original and quantised data. The smaller this value is, the better the quantised points approximate the unquantised atoms.
 \end{itemize}
\end{itemize}

\subsection{Encoder}

We chose an Arithmetic Coder to produce the actual compressed output for the compressors. Its design has the Arithmetic Coder separated into two parts: the core encoding and decoding algorithm and the adaptive or static Models. 

\subsubsection{Arithmetic Coder}

The core algorithm is only able to encode a range. This changes the state of the current range in the Arithmetic Coder and possibly outputs bits to the file. As part of the separation, it is not aware of what symbol it is compressing or decompressing.

\subsubsection{Models}

There were two different models that are used in the Arithmetic Coder: The Adaptive Arithmetic Model and the Byte Encoder. The Adaptive Arithmetic Model is an adaptive Model meaning that new symbols can be added and symbol frequencies can be updated. This Model handles all necessary jobs such as encoding the actual byte data of the symbols to the compressed file so that, on decompression, the original stream can be faithfully reconstructed. This is the main Model used in compressors.

The Byte Encoder contains only a static model and is only able to compress the byte data, or numeric values in the range 0 to 255. It is meant for simplicity, when using an Adaptive Model is unnecessary. This usually occurs when data needs to be written to the file, but no compression on these symbols is required. 

The discussion on the design of all the components has concluded. In the next section the compressed file format used for all compressors will be described.


\section{Compressed File Format}

We chose a standard file format for all compressors. This decision simplified the implementation of the compressors and allowed easy addition of new compressors. The format has several sections that allow it to reconstitute an uncompressed DCD file as can be seen in Figure \ref{fileformat}.

\paragraph{File Header}
\ \\
The header of the file is broken up into two segments. The first segment stores the DCD file's header, which includes the number of atoms and frames in the simulation. The second segment stores header information relevant to the compression. In many of the compression schemes this is simply the quantisation levels, however some of the Interframe schemes require additional information about prediction window sizes.

\paragraph{File Body} 
\ \\
The remainder of the file consists of records, one per frame. Each record contains a frame header followed by the compressed data. The frame header records information about the bounding box of each frame. Using the bounding box and the quantisation levels, a quantised approximation to the original data can be constructed. The compressed data section is specific to each compression scheme.

\begin{figure}
 \center
 \includegraphics[width=0.4\textwidth]{resources/FileFormat.png}
\caption{Compressed File format: The green blocks represent per-file header information that is stored only once. The blue blocks represent data that occur in a per frame basis.}
\label{fileformat}
\end{figure}

% \subsection{Overview}

In the following section, we will describe the design of the experiment to test the compressors.

\section{Compressor Experiment Design}

The experiments that are performed in evaluating the system are designed to test both compression rate and speed of the implementations. The experiments must be able to answer the question of which Interframe compression scheme performs the best. The main metrics that we need to gather for this experiment is the compression ratio for each file and the running time for the compression. The important question is can we are requiring to ask is which compressor performs the best. As discussed in Chapter 1, the compression ratio is a measurement of how effective the compression is. A small compression ratio implies a small compressed file. The compression ratio is calculated by dividing the size of the compressed file by the size of the original file. The running time is determined as the CPU time elapsed from the start of the compression process and the end.

The performance of the schemes will be compared by measuring the compression rate and speed, on two levels. For each dataset the compression rate and computation time will be measured. Significant changes running times can point to issues that can cause the algorithms to perform slower. A similar argument can be made for the compression rate. The causes of high variation may be determined by looking at the number of the motion of the particles, while Omeltchenko may require further information to analyse. The results and analysis of the Omeltchenko scheme may be found in the report on Intraframe compressors.

Several different datasets will be used, described in Section 5.1. The sizes of the simulations will vary from very large, a few hundred thousand atoms with one hundred to one thousand frames, to very small simulations of only a few hundred atoms and a few dozen frames, with several datasets in between. There will also be datasets that consist completely and of water and datasets that contain no water. Finally, we will also have datasets that contain a very complex molecule in solution.

\section{Summary}

In this chapter we discussed the design of the system as a whole. The various components required for compression are discussed. The compressed file format is detailed showing the different sections of the compressed file and what they are used for. Finally, the experiment design is explained. The next section will discuss the implementation details of each of these components.

\chapter{Implementation}

This chapter will describe the implementation details of each component used in the system, namely the Omeltchenko scheme, each of the Interframe compressors, components for processing the simulations into more workable formats, components for ensuring that the compression schemes produce the correct output and the Arithmetic Coder component which contains the components for performing compression.. 

As stated before, five interframe compression schemes are used in our project and their implementation is described here:
\begin{itemize}
 \item Polynomial Extrapolation
 \item Spline Extrapolation
 \item Smallest Error Encoding
 \item Common Error Encoding 
 \item k-Nearest Neighbour Encoding
\end{itemize}

The simulation processing components are split up into two categories. The I/O components handle processing and storage of uncompressed data to and from the files as well as the processing of quantisation necessary for compression. The verification components gather statistics on the effects of quantisation as well indicate whether the compression has failed and the original and uncompressed files differ too much. Finally, the Arithmetic Coder components handle the I/O of all compressed files.

\section{Intraframe Compressors}

Only one Intraframe Compression scheme, the Omeltchenko scheme, is described here. The remaining Intraframe Compression schemes' implementations are explained in the Intraframe thesis\cite{kegcomp}.

\subsection{Omeltchenko Scheme}

\label{imp_omelt}
The Omeltchenko Scheme was implemented with slight differences to the algorithm described in the paper. The main reason for these differences is that the paper describing left out details. The algorithm was initially implemented with the prescribed sorting algorithm for sorting the list of octree indices. The paper describes using a large array of lists that, for uniform distributions of atoms, maps approximately $O(1)$ atoms to each list. Each of the lists are then sorted using a heapsort and merged into a final sorted list. The size of the array is equal to the number of atoms in the frame and divides the range of octree indices into roughly equal parts. The benefit of this approach is that, for uniform distributions, all indices can be sorted in approximately linear time.  In large simulations, however, the memory usage and overhead of running many heapsorts grows quickly. In order to reduce the computation, the paper suggests splitting the molecular simulation up into smaller boxes and then merging the sorted indices of these boxes. No instruction on how to split the simulation up is provided. In our implementation, the STL sort is used. This algorithm has $O(NlogN)$ worst-case behaviour.

The encoding of each symbol in the Omeltchenko scheme is done using an adaptive encoding scheme. The details of the encoding scheme are clear, however, the criterion for adapting the various parameters is not fully explained. They do give a name to each of the adaptive parameters, but do not rigorously explore how adaptation is implemented. This forced use to make several assumptions in our implementation. There are constants used for determining how many wasted bits can be output before the frequency of status bits are changed. The paper merely states that they initialise these to certain values regardless of simulation, but do not explain why these are chosen. We choose to keep these values the same as they are in the reference paper.

\section{Interframe Compressors}

This section will described the implementation of the Interframe schemes. Each of these compressors have an adjustable parameter that allow the users to change the number of previous in the prediction window. The window, itself, was implemented as an STL deque rather than an STL queue. The main reason for this is that STL queues do not support iteration over the elements without removing elements. STL deques, on the other hand, support both constant time random-access and iteration over all elements and all the queue operations have the same complexity. The one drawback is that STL deques do not keep their elements in a single contiguous block of memory, but rather lots of smaller blocks of contiguous memory. As a result there is potential loss in spatial coherence of memory access, which will likely not cause any any problems as the windows are likely to be very small. When the window is filled, frames which are no longer necessary are popped off the front. New frames are always pushed onto the back the deque.

All of the Interframe Compressors work on the coordinate level, as opposed to the atom level. This means that each individual $x$, $y$ and $z$ coordinate is treated as separate and compressed individually. There are two reasons for this. The first is motivated by efficiency: many of the operations performed are much simpler to code when working with one dimensional data as opposed to three dimensional data and have more efficient solutions. The second reason is that it can lead to better compression. Experiments were run on certain compression schemes testing both coordinate-based and atom-based variants. The coordinate-based approach has the advantage that the number of symbols tend to be considerably lower than the atom-based with error values being shared across dimensions as seen in Figure \ref{High}

\begin{figure}
 \center
 \includegraphics[width=0.5\textwidth]{resources/HigherSymbols.png}
\caption{A prediction scheme can only generate as many symbols as the size of the space. In the atom-based variant(left), shown in 2D here, the size of the space is larger than in the coordinate-based variant. }
\label{High}
\end{figure}

\subsection{Polynomial Extrapolation}

The Polynomial Extrapolation scheme requires a method for interpolating polynomials. Our implementation uses Lagrange Polynomial Interpolation. Typically, this method requires quadratic time (in the size of the window, K) to construct and evaluate a polynomial. This needs to be done for each coordinate of each atom for every frame in the simulation. An optimisation was made to decrease the time to linear at each step. 

\begin{eqn}
\begin{eqnarray*}
 l_j(x) &=& \prod_{i=0,i \ne j} \frac{x - x_i}{x_j - x_i} \\
 F(x) &=& \sum_{j=0}^{K-1} y_jl_j(x)
\end{eqnarray*}
\caption{The functions $l_j$ represent the weight functions, while F is the output polynomial.}
\label{eqn:poly}
\end{eqn}

Equation \ref{eqn:poly} shows the equation used for the Lagrange Interpolation Polynomial. Using the fact that we only evaluate $F(K)$ at each step, we can precompute all of the $l_x$ terms because they only depend on the values of $x$. These x-values are always consecutive integers in the range $[0..K-1]$ and refer to the order of items in the prediction window. 

We calculate the denominator and numerator of each $l_x$, seperately. Substituting $x = K$ and $x_i = i$ into the numerator, yields $K!$. Precomputing and storing this puts a limit on the size of the prediction window. IEEE 754 64-bit floating point numbers can only represent factorials up to 170!, but this not a problem as window sizes are usually small. The numerators are precomputed as soon as $K$ is known and their values stored in an array. This requires us to use only linear extra space. 

Using this optimisation, the value of each $l_x$ required by $F(K)$ can be calculated in constant time by using the precomputed values.

\subsection{Spline Extrapolation}

The specific spline that is implemented is called B\'ezier curves which is another type of interpolating polynomial. \cite{10.1109/38.156018} The degree of the spine is made adjustable by modifying the prediction window size. As with Lagrange Polynomial Interpolation, the na\"ive implementation performs quadratically with the size of the prediction window. 

\begin{eqn}
\begin{eqnarray*}
 F(t) &=& \sum_{i=0}^{K-1}\  \binom{K-1}{i}(1-t)^{K-1-i}t^iy_i \\
 t &=& \frac{x}{K-1}
\end{eqnarray*}
\caption{The function F represents the Bezier curve. The function is defined in terms of $t$ when constructed. t values are real values in the range $[0..1]$, while the available x-values are in the range $[0..K-1]$. A simple conversion function is applied in order to get correct results.}
\label{eqn:spline}
\end{eqn}

The spline function is calculated with according to the equations in Equation \ref{eqn:spline}. The implementation in the system was optimised to perform linearly with the use of precomputed lookup tables. Our implementation requires the storage of multiple factorial values so it suffers from the same windows size drawback as Polynomial Extrapolation. The factorial values are used to calculate each choose function in constant time. Finally, the powers of $t$ and $1-t$ are calculated in constant time by using their previous values rather than recalculating the entire power for each iteration of the sum loop. 

The spline extrapolation at window sizes of $1$ and $2$ outputs exactly the same symbols as the Polynomial Extrapolation compression at these window sizes. This refers to delta compression or $0th$ order prediction and linear prediction or $1st$ order prediction.  

\subsection{Smallest Error Encoding}

The Smallest Error Encoding scheme requires that the prediction window be kept in sorted order. The current implementation sorts the prediction at each step which adds a significant factor to the running time. As an improvement, the window is copied into a separate data structure which keeps the items in a sorted order in an effort to improve the encoding of index symbols. 

This results in an algorithm that runs in $O(KlogK)$ per coordinate. Several enhancements could be made to reduce the running time to $O(logK)$ by maintaining the sorted window rather than recreating it every frame. However, due to time constraints, this was not implemented.  

\subsection{Common Error Encoding}

As with the Smallest Error Compression scheme, the window is sorted at every frame and leads to a large running time of $O(KlogK)$ per coordinate, however, it could be improved to $O(logK)$ in the same way.

The algorithm can be classified as being greedy. It makes an assumption that it should encode the symbol and index with the highest probablity at each step in order to get the best compression. This assumption is false, as it could be beneficial to encode symbols with lower probabilty, thus increasing their number of occurences and, hence, their probabilities. 

A more accurate solution would be to use a complete search, however this is slow as the search space has exponential size. Various enhancements could be used to accelerate the complete search, such as branch-and-bound or switching to a better method, for instance Genetic Algorithms. We leave these as an area of future work.

\subsection{k-Nearest Neighbour Encoding}

The final scheme, being based on a k-Nearest Neighbours, requires that a nearest neighbour algorithm be implemented. The initial implementation used the Approximate Nearest Neighbour(ANN) library, for calculating nearest neighbours. However, the data structures that ANN provide for accelerating the search are static and so must be reconstructed at each timestep. This adds a significant overhead to the encoding process. The final implementation uses a simple linear scan of all points for the most similar feature vector.

The similarity of two feature vectors is determined in our implementation by the Euclidean metric. Also, in most implementations the algorithm measures similarity based on upto $k$ nearest neighbours, however, in our implementation we have $k$ set permanently to $1$.

\section{Simulation I/O}

We chose to use the PDB and DCD file types exclusively as our target formats, since these are the main formats used in VMD. We had separate components for each function we required from our project. The DCD I/O component performs writing and read of the simulation files. The PDB I/O component reads the PDB files which store information on the atoms of the simulation. The Frame component acts as a storage area for simulation frames. The Quantiser is able to quantise simulation frames as well as store them.

\paragraph{DCD I/O}
\ \\
We used code from the VMD project for loading and saving of DCD files. This was mainly due to the DCD files having a complex binary format, further exacerbated by the unspecified nature of the endianess for each file. From reading the comments in the code we found that the loading of DCD files is not as efficient as possible since it requires copying to separate buffers to reorder the coordinates in the atoms. This is not a problem since it is mainly an issue with the DCD format. This does not impact the relative performance of each of our compressors as it adds a similar overhead to each file of the same size.

\paragraph{PDB I/O}
\ \\
Although there are many features of the PDB standard, only a very small subset is necessary. The PDB file is a text file that stores the data in several records. Our implementation of PDB I/O only supports reading in of a PDB file. 

We also ignore any records which are not labelled ATOM. These ATOM records store information regarding the type of each atom and what molecule it exists in, which is useful for detecting water molecules.

\paragraph{Frame}
\ \\
The frame component is simply a container that simply wraps around an STL vector. It provides convenient methods that allow it to look more like a stream of floating-point atom coordinates.

\paragraph{Quantiser}
\ \\
The quantiser was implemented as a class which wraps around a vector. The class provides methods to convert to and from a normal unquantised frame. In order to easily incorporate the Omeltchenko scheme, it adjusts the points that are being quantised to be non-negative. Non-negative indices are required by the Octree Indexing method of the Omeltchenko scheme due to the bit-wise operations that must be performed.

Objects of this class store all the information necessary to 'dequantise' the points.

\section{Simulation Processing}

The most significant task that is necessary for visualisation and the Intraframe compression schemes is the grouping of the water molecules in the file into water clusters. A water cluster is simply a collection of water molecules that are bonded together by the attractive force between the oxygen and hydrogen atoms of adjacent water molecules. 

The visualiser requires all the water clusters in order for certain visualisation modes to work. The Predictive Point Cloud Compressor requires the water clusters to use heuristic predictors. 

The task of extracting water clusters was broken in two parts: The Frame Splitter and the Cluster Extractor. Atoms within the simulation must first be identified based on what molecule they belong to. The most important group is the water molecule category. The water molecules from the Frame Splitter are sent to the Cluster Extractor. The Cluster Extractor's job is to connect the individual water molecules into water cluster groups using a graph data structure.

\subsection{Frame Splitter}

The Frame Splitting is used to break the atoms in a frame into two groups: Water molecules and non-water atoms. This is necessary for both visualisation and compression. The visualisation needs to know which atoms are water consitituents and which atoms are part of other molecules so it can apply different visualisation algorithms to both. On the compression side we need to know which atoms form water molecules for the Predictive Point Cloud Compressor so it apply the heuristics that exploit the characteristics of water molecules.

Our implementation of the Frame Splitter requires that a valid PDB file be used in order to function correctly, so if the PDB is not available or inaccessible, the Visualisation and Predictive Point Cloud Compressor will not work. 

There are other implementations of the Frame Splitter that we could have tried which do not rely on the PDB file. One alternative is to assume that every atom is part of a water molecule and assign the bonds based on the two closest atoms. This was not implemented for two reasons. Firstly, the algorithm is likely to make incorrect assumptions about what the function of atoms are in the simulations. Since the atoms in the simulation are simulated realistically, each of the atoms are likely to have react in different ways instead of all acting like water. The final reason is that VMD requires that a PDB file be loaded to view a DCD file, so the required information is always likely to be available.

The algorithm to extract the water molecules uses a series of map data structures, equation \ref{eqn:pdbmap} to connect the atoms into separate water molecules. The ATOM records loaded from the PDB file are scanned linearly. ATOM records with names other than ``OH2'', ``H1'' or ``H2'' can be safely categorised as not belonging to a water molecule. These labels correspond to the oxygen atom, first hydrogen atom and second hydrogen atom respectively. If the names do match, they are added to the maps. 


\begin{eqn}
\begin{eqnarray*}
 map[atom\ name][residue\ name][residue\ sequence\ number][segment\ id] \rightarrow index
\end{eqnarray*}
\caption{The mapping system used to determine a water molecule. Atoms in each water molecule will have the first entry as ``OH2'', ``H1'' or ``H2'', but the following entries will be exactly the same.}
\label{eqn:pdbmap}
\end{eqn}

Using the STL map, this algorithm runs in average and worst-case $O(NlogN)$ time, however it could be improved to average case linear time using a hashtable-based data structure. This was not implemented because the standard hashtable in the STL is deprecated and there was not enough time to implement a hashtable for our use.

\subsection{Cluster Extractor}

The clusters to be output from this component will be stored as a graph data structure. This graph will have several disconnected components, one for each cluster. Edges between water molecules indicate a possible direct link between two water molecules. Although this may not be physically accurate, one can adjust a tolerance setting. A low tolerance will result in many small water clusters consisting only of few water clusters. A high tolerance will have very few clusters which are a lot larger than they should be. We performed some experiments that showed that smaller tolerances are best for visualisation purposes. 

Since the core algorithm of the cluster extraction is a nearest neighbour lookup in a fixed radius, we tested two acceleration strategies. The first implementation used a three dimensional grid. The closest atom within a fixed radius to a point in space can be accomplished by examining all cells within the radius. A better technique for searching in the grid approach uses a simple breadth-first search of neighbouring cells. The fixed radius properties allows only $O(1)$ cells to be examined in the worst case, and the physical properties of the atoms mean that very few atoms will exist in each cell. Our algorithm could be improved to the breadth-first search approach, but due to the small tolerances used, the optimisation was necessary. 

The grid was implemented using the STL map data structure. The benefit of this was a decrease in the requirements of storing the grid. If a 3D array was used the memory usage could be considerably large and grow with the size of the bounding box. Different methods of minimising the size of the grid by using coarser grid cells could have been employed, but there are cases where the nearest neighbour lookup can degrade into a slower linear time operation.


\begin{eqn}
\begin{eqnarray*}
 grid[\lfloor x\rfloor][\lfloor y\rfloor][\lfloor z\rfloor] \rightarrow water\ molecules
\end{eqnarray*}
\caption{The mapping system used to determine a water molecule. Atoms in each water molecule will have the first entry as ``OH2'', ``H1'' or ``H2'', but the following entries will be exactly the same.}
\end{eqn}

Using a series of STL map data structures, the size of the grid can be made to be dependant only on the number of atoms and not the size of the bounding box. This is because the number of items in each map is proportional to number of $x,y,z$ coordinates. This quantity is bounded by the number of atoms. There is a drawback with this approach in that a cell look-up now requires logarithmic time, while the array method offers constant time cell look-up. The average case efficiency of this approach could be accelerated by using a hashtable based data structure or other spatial hashing method.

The other implementation uses ANN\cite{ann}. ANN allows us to defer the task of nearest neighbour search to a library. Although the name implies that answers given are approximate, ANN has a setting to return the exact neighbours. ANN typically uses one of two data structures to accomplish searches: KD-trees and box-decomposition trees. The KD-trees discussed in Section \ref{gdkdtree} can also be used to find the nearest neighbour. The box decomposition tree uses various rules to decompose the space into a collection of boxes, but is otherwise similar to a KD-tree. The main differences lie in how boxes are split and shrunk to form different boxes. The parameters for both the KD-tree and box-decomposition tree versions were explored. However, we found in practice that there was no difference between them. We settled for the standard KD-tree implementation because it is easier to use.

\section{Verifiers}

The verifiers were implemented as separate driver programs in the system. This separation allows the information to be collected without impacting the performance of the compression schemes. The verifiers can be run after the compression and quantisation has been performed.

The main algorithm that is implemented for this component is the algorithm for computing the variance. This is implemented using a linear algorithm, after calculating the mean.

In our implementation we collect 4 statistics for each dimension separately:
\begin{itemize}
 \item Minimum Error: 
 \begin{itemize}
   \item The smallest difference between the original and quantised data.
 \end{itemize}
 \item Maximum Error:
 \begin{itemize}
   \item The largest difference between the original and quantised data.
 \end{itemize}
 \item Mean Error:
 \begin{itemize}
   \item The average difference between the original and quantised data.
 \end{itemize}
 \item Error Variance:
 \begin{itemize}
   \item The variance of the errors statistics between the original and quantised data.  
 \end{itemize}
\end{itemize}


\section{Encoder}

Our implementation uses an Arithmetic Coder to convert streams of symbols to a compressed bitstream that can be written and read from a file. We also implemented several models that facilitate the mapping of symbols to ranges for the Arithmetic Coder can encode and vice versa.

\subsection{Arithmetic Coder}

We decided to implement our own Arithmetic Coder instead of using an existing open source package. We are building our own Arithmetic Coder for several reasons. Most Arithmetic Coders use 32-bit integers for their computations. However, this puts limits on the maximum symbol frequency and the maximum number of symbols, typically 256, as the product must fit into the data-types being used\cite{RefWorks:4}. The maximum symbol frequency is solved in most existing implementations by frequency scaling, however, the maximum number of symbols is strict and the user is left to solve it. Both of these effects can have a detrimental effect on compression. 

At the start of our project, we were unsure of the number of symbols that might be generated for compression, for instance due to a prediction scheme that performs poorly. Another reason is that the simulations can be very large with billions of symbols being encoded. Our main aim for our Arithmetic Coder was to produce an implementation that overcomes these problems.

The implementation of the Arithmetic Coder is based on the work of Bodden et. al\cite{bodden2001arithmetic}. We made several changes to their implementation Arithmetic Coders. To cope with the symbol and frequency limits, 64-bit integers are used. Although this does not eliminate range problems, they occur at such large numbers that it is safe to ignore their effects.

Several low-level changes were also made. The encoding and decoding functions are called frequently, so these functions are inlined. A lot of the range computation requries multiplying ranges by $2$, which are instead replaced by shift operators. Many micro-benchmarks were run in order to get the speed comparable to that of other available Arithmetic Coders.

A problem with our implementation is that there is only allowed to be one Arithmetic Coder writing to a file. Once an Arithmetic Coder has been initialised to write to a given file, the file cannot be written to in any other way. Also, once the Arithmetic Coder has been associated with a file it cannot be stopped until all output to the file is complete and the end of file is signalled. The reason for this is that the decoder is unaware how many bits of file actually correspond to Arithmetic Coder output after the end of file symbol is decoded. The safe assumption is taken that the entire file corresponds to Arithmetic Coder output. These issues do not present a serious problem, however the first two problems are solved by using different models for the Arithmetic Coder. The third problem does not affect us since our file format has no extraneous data after each frame of compressed data.

\subsection{Models}

A model stores the information about the current status of compression. Specifically, it needs to implicitly or explicitly store the list of symbols with their frequencies. Two different models were made for separate circumstances. These are the Adaptive Model and the Byte Model.

\subsubsection{Adaptive Model}

The Adaptive Model is a heavy-weight implementation of the adaptive parts of an Adaptive Arithmetic Coder. The Adaptive Model uses a data structure called a \emph{Fenwick Tree}\cite{fenwick1994new} to store a dynamic cumulative frequency table and a data structure called a \emph{Trie}\cite{maly1976compressed} to map the symbol bytes to positive numbers that the Fenwick Tree can work with.

\paragraph{Fenwick Trees} \ \\
The Fenwick Tree is a variant of a range tree which was specifically designed for cumulative frequency tables. This makes it especially useful for Adaptive Arithmetic Coders for representing the changing list of symbol frequencies. It works by using the fact that postive integers can be written as a sum of powers of two to answer range queries quickly. The Fenwick Tree was chosen as the main data structure for ease of coding. It also supports the required operations of updating and querying of frequencies and determining which symbols belong to a cumulative frequency range in $O(logN)$. Algorithm \ref{alg:update} shows pseudocode for updating a symbol's frequency.

Fenwick Trees do not usually support dynamically growing a list of symbols, so a modification to the data structure was performed. The Fenwick Trees are implemented in an array, so adding new symbols also requires this array to grow. The arrays are initialised with enough space to hold $1000$ symbols. When the array is full, the array size is doubled and the Fenwick Tree invariants are re-enforced by re-adding the symbols with their frequencies. This enforcing takes $O(NlogN)$ time where $N$ is the number of symbols. 

When the array is not full, symbols can be added in $O(1)$. This gives an amortised time to add new symbols of $O(logN)$. This usually does not cause an issue as the number of symbols in the model tends to be small. 


\begin{algorithm}                      % enter the algorithm environment
\caption{Update a symbol's frequency.}          % give the algorithm a caption
\label{alg:update}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}                    % enter the algorithmic environment
\WHILE{symbol $\leq$ tableSize}
\STATE table[symbol] $\Leftarrow$ table[symbol] + 1
\STATE symbol $\Leftarrow$ symbol \& -symbol
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\paragraph{Tries} \ \\
The Fenwick Tree only supports symbols labelled from $[1..N]$ so an additional mapping is required to convert to and from this representation which requires data structures. The data structures must map byte strings to symbol numbers for the encoding process and symbol numbers to byte strings for the decoding process. The Adaptive Model is designed to have symbols which are arbitrary length byte strings so any data structure that we implement must be able to handle this. 

The encoding mapping originally used an STL map. This has worst case performance which is logarithmic in the size of the symbol table, however, there are hidden costs due to string comparison, which is not a constant operation. The current implementation uses a Trie data structure to accomplish the mapping. A Trie is tree data structure where the edges are labelled as byte values and paths from the root to a leaf represent items. Figure \ref{Trie} shows an example Trie containing English words.

\begin{figure}
 \center
 \includegraphics[width=0.4\textwidth]{resources/trie.pdf}
\caption{A graph representation of a Trie containing the words: care, car, cat, cow and cook. Shaded vertices indicate that it is the last letter of some word. Vertex labels indicate the words or partial words contained in the Trie. Edge labels indicate letters that, when added to their parent's label represents a new word.}
\label{Trie}
\end{figure}

Tries keep their data in sorted order and allow roughly constant time access to each item. They technically require linear time proportional to the number of bytes in the byte string. Tries tend not to perform well with large numbers of entries due to the large number of memory allocations and a lack of spatial locality, but for the application of symbol tables they aren't expected to grow very large. 

For the decoding mapping an STL vector was used. New symbols are appended to the vector and can be looked up by using the symbol ID from the Fenwick Tree as in index into the array.

\paragraph{Symbol Table Encoding} \ \\

The main problem with the Adaptive Arithmetic Coders, is that the symbol table needs to be stored in the file, so that the file can be reconstructed correctly. It is not possible to know the symbols which should be encoded beforehand, as this requires substantial preprocessing and limits all of our compression to off-line processes. This is solved in our implementation by outputting new symbols while the encoding process is occuring. This technique is similar to Vitter's algorithm for Adaptive Huffman Compression, however, unlike Vitter's algorithm the bit pattern for each symbol cannot be simply output. This is because there is more state in an Arithmetic Coder than Huffman Coder, specifically the current range. For this reason, outputting of the bits needs to be performed through the Arithmetic Coder using a model. 

The Adaptive Model contains an escape symbol which signals that the next symbols to follow represents the byte string. The byte string is output via a separate static model. The model implicitly stores a symbol table which contains $257$ symbols. The first $256$ symbols are used for the encoding the individual byte values in the byte string. The last symbol is used as a terminator to indicate that the byte string has ended and normal symbol encoding should continue. New symbols can be handled on the decoding side by following a similar process.

\subsubsection{Byte Model}

The Byte Model is a light implementation of a static section of the Static Arithmetic Coder. It features an implicit static symbol table of $256$ elements and all the frequencies are kept at a constant value of $1$. The symbol values correspond to byte values between $0$ and $255$. The effect of this model is that bytes can be written out in an uncompressed form to the file using the Arithmetic Coder. The benefit of this model is that raw bytes can be written without storing a symbol table in the Arithmetic Coder, thus saving space. 

\section{Summary}

In this chapter we discussed the implementation details of various components in the system. We explained our implementation of the Omeltchenko Intraframe scheme along with the assumptions and subsequent modifications that were made. The various Interframe compression schemes, Polynomial Extrapolation, Spline Extrapolation, Smallest Error Encoding, Common Error Encoding and k-Nearest Neighbour Encoding, were detailed. 

Simulations processing components were analysed including the various tested implementations of the Frame Splitter and Cluster Extractor. The statistics gathered in the Verifiers were analysed. Finally, the Arithmetic Coder implementation was detailed along with the various Models used in the compression. The following section will discuss the results from the experiment.

\chapter{Results}

The testing process was performed on a PC using a single Intel Xeon 3.00GHz processor and 3GB of RAM. Each compressor was run on several data sets. 

NAMD is a program, included with VMD, that is able to produce simulations that are compatible with our project. The program requires three files to be present in order to run the simulation. The first is the PDB file which determines which atoms are present in the simulation. The PDB files cannot be automatically generated as they correspond to positions of atoms in an actual molecule. Several PDB files were downloaded from the RCSB, which maintains archives of many free PDB files\cite{rcsb}. Finally, an important function that VMD can perform is to modify PDB files in order to embed the molecules in water. This was used to increase the number of atoms in several simulations.

The second file is the PSF file which contains information on the bonds between atoms in the PDB file. VMD can generate this file automatically from a given PDB file. 

Finally, a configuration is necessary that allows the user to set various parameters for the simulation. There are two important settings that control the granularity and length of the simulations. These settings control the total number of simulation steps and how many simulations steps are performed for each step that is output to file. The greater the granularity, the less coherent the steps in the simulations, but the smaller the size of the output file. One last setting that affects the motion in the simulation is the initial temperature at which the simulation is run.

In the next section, the datasets that were used for experiments will be discussed.

\begin{section}{Datasets}

A total of 16 datasets were used in the experiment, however, only a representative set of 7 will be displayed here. These datasets represent a range of parameters such as run fine granularity or high temperatures. 

\begin{itemize}
 \item HIV: This is a small dataset which represents part of the Human Immunodeficiency Virus (HIV) surrounded by water. It consists of 6345 atoms simulated over 200 frames and run at room temperature (25 degrees celcius). For each output frame in the file, there are 50 frames of simulation.
 \item Water: This dataset consists completely of water molecules. There are 96,603 atoms simulated over 220 frames. The simulation is run at room temperature and for each output frame there are 50 frames of simulation.  
 \item Hot Water: This dataset is the same as the Water dataset, but at a much higher temperature. The temperature is run at 1000 degrees kelvin.
 \item MSCL: MSCL is a medium-sized dataset that was donated by Dr. James Gain. It contains two volumes of water, separated by a protein called Mechanosensitive Channels (MSCL). This is a protein is responsible for opening pores in response to mechanical stress. It is believed to be a vital part in many of our senses. It contains over 100,000 atoms simulated over 300 frames. The output rate is unknown, but from looking at the simulation it would seem that the motion of the water molecules is quite erratic.
 \item Small Water: This is a very small dataset which also contains only water. There are 699 atoms and 100 frames of data. The simulation is run at a very fine granularity of 1 frame of simulation for each output frame. The simulation is run at room temperature.
 \item Rabies: This is the Rabies Virus, which looks similar to the sides of a cylinder. At each end of the molecule there are two areas of water which cover the openings. The simulation contains 175,790 atoms and 1,200 frames. The simulation is run at room temperature and output at 1 simulation frame per output frame. There is very little motion in this simulation, as there is only a small area for the water and virus to interact. This results in motion that is very coherent.
 \item Polysaccharide: A polysaccharide is a carbohydrate that is an important biological polymer(a long chain of molecules). This dataset was donated by Dr. Michelle Kuttel. There are only 1,509 atoms in this simulation, but there are over 100,000 frames. The granularity and temperature are unknown, but from inspecting the simulation in VMD, one can see that the motion is very erratic and sometimes appears to jump significantly.
\end{itemize}

\begin{table}[!h]
\begin{center}
        \begin{tabular}{ | l | l | l | l |}
                \hline
                Dataset & Atoms & Frames & Coherency \\ \hline
                HIV & 6435 & 200 & Normal \\ \hline
		Water & 96603 & 220 & Normal \\ \hline
		Hot Water & 96603 & 200 & Normal \\ \hline
		MSCL & 111016 & 300 & Erratic \\ \hline
		Small Water & 699 & 100 & Consistent \\ \hline
		Rabies & 175790 & 1200 & Consistent \\ \hline
		Polysaccharide & 1509 & 109091 & Erratic \\ \hline
        \end{tabular}
\end{center}
\label{tableplane}
\caption{Table indicating the properties of each of our dataset. Erratic simulations have very jumpy motion, while consistent simulations have very ordered motion. Normal motion is taken to be motion that is not coherent or erratic. In the case of the MSCL and Polysaccharide dataset, the motion was determined by playing the simulation in VMD.}
\end{table}

Table \ref{tableplane} represents the properties of each of the simulation. The next section will show the results from the experiments that were performed.

\section{Experiment Results}

Several statistics were gathered from running the experiments: The compression size, compression time and decompression time. The decompression time is not shown in the tables below as it follows a similar pattern to the encoding time. One difference is that the decoding is typically slower, by 20\% to 25\% due to the Arithmetic Decoder not being optimised as heavily as the Arithmetic Encoder. 

A selection of window sizes were chosen for each compressor. Polynomial Extrapolation, Spline Extrapolation, Smallest Error Encoding and Common Error Encoding have one input parameter which is the window size. The values chosen were: K = 1, 2, 3, 5 and 10. These values were chosen because they represent a range of windows based on small, medium and large sizes. In the case of k-Nearest Neighbour Encoding there are two parameters that can vary, the size of the window and the dimension of the feature space vectors. The window sizes of 2, 3, 4, 5 and 10 were chosen, but for each of the sizes a different set of feature vector sizes was chosen. The size of the feature vectors allowed is 2, 3, 4 and 9. However, there is an added restriction that each feature vector size must be strictly less than the window size.

These tests were run using several quantisation levels, namely 8, 12 and 16 bit quantisation across all the dimensions. The results are similar across all quantisation levels. In order to save space, only the 8-bit quantisation is shown here. The difference between results of the different quantisation levels is that a higher quantisation corresponds to a lower compression ratio and a longer compression time. The relative ordering of the results for differing parameters remains the same. The full results may be found in Appendix A.

\subsection{Polynomial Extrapolation}

%INTERFRAME

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=1 & & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04\% & \textbf{8.37\%} & 2.01 & 10.17\% & 2.12 & 12.68\% & 2.33 & 18.16\% & 2.76 & 27.64\% & 3.73 \\ \hline
                Water  & 243.22 & 25\% & \textbf{4.82\%} & 33.32 & 6.88\% & 33.72 & 9.42\% & 37.69 & 14.46\% & 44.69 & 26.58\% & 62 \\ \hline
                Hot Water & 221.11 & 25\% & \textbf{5.77\%} & 30.67 & 7.62\% & 31.07 & 10.23\% & 35.1 & 15.75\% & 41.11 & 26.68\% & 56.15 \\ \hline
                MSCL & 381.15 & 25\% & \textbf{9.46\%} & 57.14 & 11.61\% & 58.12 & 14.23\% & 63.21 & 19.67\% & 75.04 & 27.77\% & 98.42 \\ \hline
                Small Water & 0.8 & 25.35\% & \textbf{3.57\%} & 0.15 & 5.31\% & 0.12 & 7.77\% & 0.13 & 12.43\% & 0.16 & 24.02\% & 0.23 \\ \hline
                Rabies & 2414.14 & 25\% & \textbf{1.19\%} & 281.89 & 2\% & 305.03 & 2.88\% & 332.34 & 4.4\% & 386.95 & 9.75\% & 524.77 \\ \hline
                Polysaccharide & 1886.41 & 25.17\% & \textbf{10.62\%} & 259.99 & 12.94\% & 287.46 & 15.59\% & 313.65 & 20.97\% & 377.02 & 28.14\% & 497.27 \\ \hline
        \end{tabular}}
\end{center}
\caption{Compression information for \textbf{Polynomial Extrapolation Prediction} using different prediction window parameters. The first column represents the name of the dataset. The second column represents the size of the original file in MB. The third column shows the final size from quantisation only as a percentage of the original size. The remaining columns show two numbers: The first number is the compressed file sizes as a percentage of the original, the second number represents the time in seconds to compress the file.}
\label{intres}
\end{table}

As can be seen in Table \ref{intres}, the best parameters for the Polynomial Extrapolation occur for small parameter values. In general, the best compression scheme is for K = 1. This corresponds to the delta encoding of coordinates. As the prediction window size is increased, the compression worsens. This is because the polynomials created from interpolation tend to oscillate as their degree increases. This oscillation leads to very poor predictions which results in very large, uncommon errors being encoded. Additionally, the extra work required for constructing larger degree polynomials contribute a small but significant amount to the running time.

The scheme tends to do better for simulations with very stable motion. The scheme is able to acheive a compression ratio of 1.19\% on very coherent simulations, such as the Rabies dataset. The datasets containing only water molecules are compressed better than simulations that also contain other molecules as well. This could be due to the interaction between the water and the molecule, in these areas the atoms' motion is less predictable due to the complex interactions involved.

%SPLINE
\subsection{Spline Extrapolation}

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04\% & \textbf{10.86\%} & 2.25 & 11.55\% & 2.54 & 12.44\% & 3.2 \\ \hline
                Water & 243.22 & 25\% & \textbf{7.41\%} & 36.34 & 8.34\% & 40.98 & 9.41\% & 52.22 \\ \hline
                Hot Water & 221.11 & 25\% & \textbf{8.24\%} & 33.74 & 9.14\% & 37.87 & 10.18\% & 47.91 \\ \hline
                MSCL & 381.15 & 25\% & \textbf{12.43\%} & 62.92 & 13.11\% & 69.18 & 13.78\% & 86.46 \\ \hline
                Small Water & 0.8 & 25.35\% & \textbf{5.7\%} & 0.13 & 7.63\% & 0.15 & 9.93\% & 0.2 \\ \hline
                Rabies & 2414.14 & 25\% & \textbf{2.08\%} & 332.73 & 2.61\% & 379.16 & 2.96\% & 492.64 \\ \hline
                Polysaccharide & 1886.41 & 25.17\% & \textbf{13.74\%} & 307.19 & 14.37\% & 344.14 & 14.86\% & 433.15 \\ \hline
        \end{tabular}}
\caption{Compression information for \textbf{Spline Extrapolation} using different prediction window parameters. The format is the same as in Table \ref{intres}.}
\end{center}
\label{res_spline}
\end{table}

Table 5.2 provides similar results in terms of the best window size. The columns for K = 1 and K = 2 are not shown in the table as they correspond to delta encoding and first order prediction respectively. To save space these results can be found in Table \ref{intres}. The best scheme, in terms of general compression ratios still corresponds to a window size of 1, or delta encoding. The higher order splines do worse than simple delta compression, but when compared to Polynomial Extrapolation they do significantly better. This is because, although it is not a particularly effective predictor of atomic motion, it does not oscillate as much. There is also less work required in terms of floating point operations, so the speed of the predictor is faster.

Again, the scheme does better for simulations that are coherent. Although the scheme is as effective as delta encoding on Rabies, it achieves a compression ratio 2.08\% for a window size of 3, while the Polynomial Extrapolation has a 2.88\% compression ratio. The water-only simulations also compress better than mixed simulations.

%NEAREST
\subsection{Smallest Error Encoding}

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04\% & \textbf{10.35\%} & 5.05 & 11.48\% & 6.53 & 12.94\% & 8.12 & 15.02\% & 9.9 \\ \hline
                Water & 243.22 & 25\% & \textbf{6.29\%} & 81.21 & 7.42\% & 105.41 & 9.11\% & 129 & 11.65\% & 162.72 \\ \hline
                Hot Water & 221.11 & 25\% & \textbf{7.39\%} & 74.11 & 8.59\% & 97 & 10.32\% & 119.73 & 12.81\% & 161.32 \\ \hline
                MSCL & 381.15 & 25\% & \textbf{11.1\%} & 134.56 & 12.11\% & 173.8 & 13.4\% & 216.27 & 15.26\% & 278.41 \\ \hline
                Small Water & 0.8 & 25.35\% & \textbf{5.31\%} & 0.27 & 6.88\% & 0.35 & 9.22\% & 0.42 & 13.19\% & 0.52 \\ \hline
                Rabies & 2414.14 & 25\% & \textbf{1.56\%} & 750.03 & 1.95\% & 977.8 & 2.63\% & 1194.86 & 4.12\% & 1492.62 \\ \hline
                Polysaccharide & 1886.41 & 25.17\% & \textbf{12.03\%} & 663.84 & 12.87\% & 776.39 & 13.91\% & 1072.1 & 15.37\% & 1351 \\ \hline
        \end{tabular}}
\end{center}
\caption{Compression information for \textbf{Smallest Error Encoding} using different prediction window parameters. The format is the same as in Table \ref{intres}.}
\label{res_small}
\end{table}

The table (Table \ref{res_small}) for the Smallest Error Encoding scheme gives results similar to other schemes. The higher the prediction window, the worse the scheme does. Again, the best result is when K = 1, which is not shown on the table as it corresponds almost precisely to delta encoding. There is a difference here, however, in that index information must also be encoded for each prediction. This makes it actually perform worse than delta encoding by a few bytes. 

As can be seen in the table, the higher the window size, the worse the compression becomes. This is due to the fact that the indices always lie in the range $0..K-1$. As the window size increases the range of symbols increases. The probability of these symbols being encoding is very similar, instead of a single symbol having a high probability leading to an increase in compression ratio. This offsets any gain from using a larger window. Finally, since the current implementation sorts many times, the compression time increases considerably as the window size is increased.

For the simulations with coherent motion the scheme performs very well compared with to Polynomial Extrapolation. However, it performs worse than Spline Extrapolation. Also, although water-only simulations perform well at low window sizes when compared to mixed simulations, at higher window sizes the compression rates tend to be less distinguishable.

%COMMON
\subsection{Common Error Encoding}

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04\% & \textbf{8.99\%} & 4.1 & 9.44\% & 5.71 & 10.18\% & 7.22 & 11.41\% & 11.4 \\ \hline
                Water & 243.22 & 25\% & \textbf{5.3\%} & 62.7 & 5.68\% & 79.04 & 6.3\% & 124.4 & 7.28\% & 163.88 \\ \hline
                Hot Water & 221.11 & 25\% & \textbf{6.48\%} & 60.17 & 7.13\% & 74.23 & 8.29\% & 107.43 & 10.34\% & 175.61 \\ \hline
                MSCL & 381.15 & 25\% & \textbf{9.79\%} & 123.66 & 10.14\% & 141.25 & 10.77\% & 203.88 & 11.92\% & 308.53 \\ \hline
                Small Water & 0.8 & 25.35\% & \textbf{4.66\%} & 0.21 & 5.58\% & 0.27 & 7.05\% & 0.35 & 9.89\% & 0.54 \\ \hline
                Rabies & 2414.14 & 25\% & \textbf{1.4\%} & 560.72 & 1.61\% & 709.05 & 1.9\% & 1006.62 & 2.47\% & 1395.88 \\ \hline
                Polysaccharide & 1886.41 & 25.17\% & \textbf{10.71\%} & 582.64 & 10.9\% & 692.06 & 11.26\% & 1049.7 & 11.92\% & 1724.53 \\ \hline
        \end{tabular}}
\end{center}
\caption{Compression information for \textbf{Common Error Encoding} using different prediction window parameters. The format is the same as in Table \ref{intres}.}
\label{res_common}
\end{table}

Table \ref{res_common} shows the results for the Common Error Encoding scheme. The column K = 1, is not shown as it corresponds to K = 1 in the Smallest Error Encoding scheme, which in turn corresponds almost directly to delta encoding. The table shows that the Common Error scheme always does better than the Smallest Error Encoding scheme. Again, a noticable increase in compression ratio can be seen as the window size increases. Even though this scheme is an improvement on the Smallest Error scheme, the improvements are not good enough to overcome the impact of encoding indices. The scheme does best in terms of having the slowest compression ratio growth among the interframe prediction schemes, although it also has one of the worst compression times on average, due to the sorting and additional data structures that are required. 

The scheme has the same benefits as Smallest Error Encoding, but tends to do much better on average at each prediction window size. Simulations with very stable motion are compressed very well. The scheme has the second best performance for encoding the Rabies dataset at 1.4\%. A similar effect is seen in Smallest Error Encoding where the water-only simulations are compressed much better at low window sizes. This benefit also disappears at higher window sizes.

\subsection{k-Nearest Neighbour Encoding} 
%NN part 1

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3;V=1 & & K=4;V=1 & & K=4;V=2 & \\ \hline
                HIV & 14.53 & 25.04\% & \textbf{9.91\%} & 1.84 & 10.53\% & 1.99 & 9.97\% & 1.92 \\ \hline
                Water & 243.22 & 25\% & \textbf{6\%} & 28.64 & 6.69\% & 31.21 & 6.1\% & 29.78 \\ \hline
                Hot Water & 221.11 & 25\% & \textbf{7.3\%} & 26.9 & 8.15\% & 29.32 & 7.4\% & 27.74 \\ \hline
                MSCL & 381.15 & 25\% & \textbf{10.56\%} & 50 & 11.18\% & 53.92 & 10.61\% & 51.46 \\ \hline
                Small Water & 0.8 & 25.35\% & \textbf{5.71\%} & 0.11 & 7.05\% & 0.12 & 6.06\% & 0.11 \\ \hline
                Rabies & 2414.14 & 25\% & \textbf{1.65\%} & 264.14 & 2.05\% & 281.85 & 1.67\% & 273.08 \\ \hline
                Polysaccharide & 1886.41 & 25.17\% & \textbf{11.35\%} & 245.19 & 11.88\% & 264.65 & \textbf{11.35\%} & 253.29 \\ \hline
        \end{tabular}}
\end{center}
\caption{Compression information for \textbf{k-Nearest Neighbour Encoding} using K = 3 and K = 4 as our window sizes. The format is the same as in Table \ref{intres}.}
\label{res_nn1}
\end{table}

%NN part 2

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=5;V=1 & & K=5;V=2 & & K=5;V=3 & \\ \hline
                HIV & 14.53 & 25.04\% & 10.63\% & 2.12 & 10.53\% & 2.11 & \textbf{10.04\%} & 2.01 \\ \hline
                Water & 243.22 & 25\% & 6.82\% & 32.81 & 6.76\% & 32.96 & \textbf{6.19\%} & 32.33 \\ \hline
                Hot Water & 221.11 & 25\% & 8.34\% & 30.97 & 8.07\% & 30.87 & \textbf{7.5\%} & 28.67 \\ \hline
                MSCL & 381.15 & 25\% & 11.3\% & 57.52 & 11.17\% & 56.45 & \textbf{10.66\%} & 53.01 \\ \hline
                Small Water & 0.8 & 25.35\% & 7.36\% & 0.13 & 7.19\% & 0.13 & \textbf{6.4\%} & 0.12 \\ \hline
                Rabies & 2414.14 & 25\% & 2.03\% & 297.93 & 2.07\% & 301.25 & \textbf{1.69\%} & 282.53 \\ \hline
                Polysaccharide & 1886.41 & 25.17\% & 11.92\% & 280.21 & 11.84\% & 276.67 & \textbf{11.35\%} & 259.61 \\ \hline
        \end{tabular}}
\end{center}
\caption{Compression information for \textbf{k-Nearest Neighbour Encoding} using K = 5 as our window size. The format is the same as in Table \ref{intres}.}
\label{res_nn2}
\end{table}

% NN part 3

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=10;V=1 & & K=10;V=2 & & K=10;V=3 & & K=10;V=4 & \\ \hline
                HIV & 14.53 & 25.04\% & \textbf{10.99\%} & 3.33 & 11.2\% & 3.25 & 11.31\% & 3.46 & 11.23\% & 3.59 \\ \hline
                Water & 243.22 & 25\% & \textbf{7.16\%} & 48.03 & 7.33\% & 52.19 & 7.5\% & 56.8 & 7.49\% & 59.67 \\ \hline
                Hot Water & 221.11 & 25\% & 8.92\% & 46.33 & 8.83\% & 48.8 & 8.76\% & 52.3 & \textbf{8.68\%} & 54.66 \\ \hline
                MSCL & 381.15 & 25\% & \textbf{11.58\%} & 83.34 & 11.69\% & 88.26 & 11.75\% & 94.92 & 11.71\% & 99.15 \\ \hline
                Small Water & 0.8 & 25.35\% & 8.76\% & 0.24 & \textbf{8.74\%} & 0.2 & 8.85\% & 0.21 & 8.77\% & 0.22 \\ \hline
                Rabies & 2414.14 & 25\% & 2.19\% & 417.72 & \textbf{2.12\%} & 481.66 & 2.18\% & 529.74 & 2.19\% & 557.83 \\ \hline
                Polysaccharide & 1886.41 & 25.17\% & \textbf{12\%} & 373.49 & 12.1\% & 428.81 & 12.19\% & 458.1 & 12.19\% & 476.76 \\ \hline
        \end{tabular}}
\end{center}
\caption{Compression information for \textbf{k-Nearest Neighbour Encoding} using K = 10 as our window size. The format is the same as in Table \ref{intres}.}
\label{res_nn3}
\end{table}

Tables \ref{res_nn1}, \ref{res_nn2} and \ref{res_nn3} show the results for the k-Nearest Neighbour Encoding scheme. These results are slightly more difficult to analyse as there are two parameters that vary. There are also many different choices of parameters that correspond to delta encoding. In general, if V = K - 1, then the encoding represents delta compression. In this regard, K=2;V=1, K=3;V=2, K=4;V=3, K=5;V=4 and K=10,V=9 were removed from the above tables to save space. 

The results show the same trend that occurs in the other schemes. As K increases, the compressed file size increases. However, as V increases for each K value, compression ratio decreases and yields better the compression. There is an interplay of choice between V and K because as K increases the difference between the compressed sizes decreases of all V choices decreases. This causes many of the best choices of values to fluctuate between a high and low V value.

This scheme, like the Smallest Error Encoding and Common Error Encoding, requires that two symbols are output per atom. It tends to perform on par with the Smallest Error Encoding at low window sizes, but at high window sizes the growth rate means that it can do worse. The best value, however, coincides with delta compression as V approaches K - 1. This scheme performs faster than Smallest Error Encoding and Common Error Encoding, but slower than both Polynomial Extrapolation and Spline Extrapolation.

Simulations with very stable motion are compressed moderately well at low prediction levels. This scheme performs poorly on almost every other dataset. Water-only simulations are also compressed moderately well at low window size, but for mixed datasets it does worse still.

\end{section}

\section{Summary}

From the results, it can be seen that delta encoding performs the best among all Interframe Compression schemes. All of the schemes follow the trend that as the window size increases, there is an increase in the time required to perform the compression, as well as an increase in the compressed file size. This decrease in overall compression occurs because the motion in the file is generally not coherent, it is simulated at a much faster rate than it is output. There are also complex interactions that tend to make the prediction-based schemes less accurate, while the non-prediction schemes also perform poorly due to encoding of additional indices.

Simulations that compressed well had very coherent motion. These simulations were compressed to around 5\% at the 8-bit quantisation level. The results were similar at the 12-bit and 16-bit quantisation levels achieving around 15\% and 40\% average compression, respectively. This non-linear increase is due to the exponential effects of quantisation. At 8-bit quantisation, very slight motion is removed, while at 12-bit and 16-bit quantisation there is enough fidelity for these motions to be represented.

There is another detrimental effect that comes with increasing the prediction window size. The Intraframe schemes must be run on enough initial frames to fill the prediction window. An increased window size will require more frames to be compressed with Intraframe prediction. This could lead to an increase in compression time and compressed file size depending on what Intraframe scheme is chosen. 

\chapter{Conclusion}

We successfully designed and implemented a system for testing the effects of different compression schemes on water dominant molecular simulations. The molecular simulation dataset was chosen based on several properties that are present in simulations, namely granularity, temperature and size of the dataset in number of atoms and number of frames.

The compression schemes that were tested fall into two categories: Intraframe compression and Interframe compression. The former category has schemes which exploit the spatial and chemical properties of the simulation, while the latter category has schemes which exploit the temporal properties of the simulation. Intraframe compression is the focus of this report.

There were five Interframe compression schemes were implemented and tested:

\begin{itemize}
 \item Polynomial Extrapolation which uses polynomial interpolation for predicting the next location of an atom.
 \item Spline Extrapolation which performs the same task as Polynomial Extrapolation, but uses spline interpolation for prediction.
 \item Smallest Error Encoding which encodes both an index into the the prediction window and an error offset, such that the errors sizes are minimised.
 \item Common Error Encoding which, like the previous scheme, encodes an index into the prediction window and an error offset, however, it uses an heuristic to estimate the effects of encoding items from the window and greedily chooses the best one.
 \item k-Nearest Neighbour Encoding is loosely based on a machine classification algorithm that attempts to encode the best result. 
\end{itemize}

Each of these schemes was tested for low, medium and high window sizes ranging from between 1 and 10 previous frames. 

From results presented in the Intraframe thesis\cite{kegcomp}, the Intraframe compression schemes typically take longer in order to compress a frame and yield worse results. The main reason for this, as discussed in the Intraframe thesis, is that the order of atoms is usually destroyed which means that extra information must be compressed to recover it. Most of an Intraframe compressed file's size comes from this ordering information.

The Interframe compression does not suffer from this limitation, as it compresses the atoms in the order they appear in the simulation file. Our results show that the best choice for parameters and compression scheme correspond to delta encoding of the atoms where typically the compression produces a file that is 10\% of the original size. The best running time also occurs for delta encoding. However, the results for the time of delta compression recorded in the previous chapter do not necessarily show the true running time of a system specifically designed for this compression which can be more heavily optimised. 

\section{Future Work}

There are several aspects on the compression side that have been left for future work. Firstly, several of the prediction schemes, namely the Smallest Error Encoding and the Common Error Encoding have simple implementations that are not heavily optimised, and can be made faster by using sorted data structures, instead of sorting the frames at each step. 

The current system is also not stringent in terms of memory usage. The next generation of parallel and distributed computers will be able to generate simulations which are 10 to 100 times larger than those in our dataset. This will put a larger memory constraint on the compressors, which sometimes require multiple copies of the frames to operate, for instance in Interframe compression. This could be overcome by using a caching system that stores only single copies of each frame, but makes it available for each part of the system to use. Various schemes, such as delta encoding, can also be made to operate using much less memory than they currently consume.

One of the compression schemes, the Common Error Encoding scheme, uses an heuristic function to approximate the effects of performing a compression. This approximation is then acted on in a greedy fashion. However better compression could be acheived by performing a complete search of the space. This search space can grow exponentially, so better exact algorithms such as branch-and-bound or approximation algorithms such as Genetic Algorithms or Simulated Annealing could be used instead.

There is a lot of scope in improving the compression with new schemes. The entire simulation can be determined from only a set of initial atom positions and velocities. One possible Interframe compression scheme is to only compress only certain frames in the simulation along with the velocities of the atoms in the simulation. The compressors could then discard all the remaining frames. On decompression, a low resolution simulation can be run from each compressed frame in order to determine the remaining, discarded frames before the next compressed frame. This could potentially lead to a significant compression rate at the cost of an increase in decompression time.

\bibliography{report_julian}

\appendix

\chapter{Results}

\section{8-bit Quantisation}

\subsection{Polynomial Extrapolation}
\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=1 & & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04 & 8.37 & 2.01 & 10.17 & 2.12 & 12.68 & 2.33 & 18.16 & 2.76 & 27.64 & 3.73 \\ \hline
                Water  & 243.22 & 25 & 4.82 & 33.32 & 6.88 & 33.72 & 9.42 & 37.69 & 14.46 & 44.69 & 26.58 & 62 \\ \hline
                Hot Water & 221.11 & 25 & 5.77 & 30.67 & 7.62 & 31.07 & 10.23 & 35.1 & 15.75 & 41.11 & 26.68 & 56.15 \\ \hline
                MSCL & 381.15 & 25 & 9.46 & 57.14 & 11.61 & 58.12 & 14.23 & 63.21 & 19.67 & 75.04 & 27.77 & 98.42 \\ \hline
                Small Water & 0.8 & 25.35 & 3.57 & 0.15 & 5.31 & 0.12 & 7.77 & 0.13 & 12.43 & 0.16 & 24.02 & 0.23 \\ \hline
                Rabies & 2414.14 & 25 & 1.19 & 281.89 & 2 & 305.03 & 2.88 & 332.34 & 4.4 & 386.95 & 9.75 & 524.77 \\ \hline
                Polysaccharide & 1886.41 & 25.17 & 10.62 & 259.99 & 12.94 & 287.46 & 15.59 & 313.65 & 20.97 & 377.02 & 28.14 & 497.27 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

%SPLINE
\subsection{Spline Extrapolation}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04 & 10.86 & 2.25 & 11.55 & 2.54 & 12.44 & 3.2 \\ \hline
                Water & 243.22 & 25 & 7.41 & 36.34 & 8.34 & 40.98 & 9.41 & 52.22 \\ \hline
                Hot Water & 221.11 & 25 & 8.24 & 33.74 & 9.14 & 37.87 & 10.18 & 47.91 \\ \hline
                MSCL & 381.15 & 25 & 12.43 & 62.92 & 13.11 & 69.18 & 13.78 & 86.46 \\ \hline
                Small Water & 0.8 & 25.35 & 5.7 & 0.13 & 7.63 & 0.15 & 9.93 & 0.2 \\ \hline
                Rabies & 2414.14 & 25 & 2.08 & 332.73 & 2.61 & 379.16 & 2.96 & 492.64 \\ \hline
                Polysaccharide & 1886.41 & 25.17 & 13.74 & 307.19 & 14.37 & 344.14 & 14.86 & 433.15 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\newpage
\subsection{Smallest Error Encoding}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04 & 10.35 & 5.05 & 11.48 & 6.53 & 12.94 & 8.12 & 15.02 & 9.9 \\ \hline
                Water & 243.22 & 25 & 6.29 & 81.21 & 7.42 & 105.41 & 9.11 & 129 & 11.65 & 162.72 \\ \hline
                Hot Water & 221.11 & 25 & 7.39 & 74.11 & 8.59 & 97 & 10.32 & 119.73 & 12.81 & 161.32 \\ \hline
                MSCL & 381.15 & 25 & 11.1 & 134.56 & 12.11 & 173.8 & 13.4 & 216.27 & 15.26 & 278.41 \\ \hline
                Small Water & 0.8 & 25.35 & 5.31 & 0.27 & 6.88 & 0.35 & 9.22 & 0.42 & 13.19 & 0.52 \\ \hline
                Rabies & 2414.14 & 25 & 1.56 & 750.03 & 1.95 & 977.8 & 2.63 & 1194.86 & 4.12 & 1492.62 \\ \hline
                Polysaccharide & 1886.41 & 25.17 & 12.03 & 663.84 & 12.87 & 776.39 & 13.91 & 1072.1 & 15.37 & 1351 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\subsection{Common Error Encoding}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 25.04 & 8.99 & 4.1 & 9.44 & 5.71 & 10.18 & 7.22 & 11.41 & 11.4 \\ \hline
                Water & 243.22 & 25 & 5.3 & 62.7 & 5.68 & 79.04 & 6.3 & 124.4 & 7.28 & 163.88 \\ \hline
                Hot Water & 221.11 & 25 & 6.48 & 60.17 & 7.13 & 74.23 & 8.29 & 107.43 & 10.34 & 175.61 \\ \hline
                MSCL & 381.15 & 25 & 9.79 & 123.66 & 10.14 & 141.25 & 10.77 & 203.88 & 11.92 & 308.53 \\ \hline
                Small Water & 0.8 & 25.35 & 4.66 & 0.21 & 5.58 & 0.27 & 7.05 & 0.35 & 9.89 & 0.54 \\ \hline
                Rabies & 2414.14 & 25 & 1.4 & 560.72 & 1.61 & 709.05 & 1.9 & 1006.62 & 2.47 & 1395.88 \\ \hline
                Polysaccharide & 1886.41 & 25.17 & 10.71 & 582.64 & 10.9 & 692.06 & 11.26 & 1049.7 & 11.92 & 1724.53 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\subsection{k-Nearest Neighbour Encoding}
\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3;V=1 & & K=4;V=1 & & K=4;V=2 & \\ \hline
                HIV & 14.53 & 25.04 & 9.91 & 1.84 & 10.53 & 1.99 & 9.97 & 1.92 \\ \hline
                Water & 243.22 & 25 & 6 & 28.64 & 6.69 & 31.21 & 6.1 & 29.78 \\ \hline
                Hot Water & 221.11 & 25 & 7.3 & 26.9 & 8.15 & 29.32 & 7.4 & 27.74 \\ \hline
                MSCL & 381.15 & 25 & 10.56 & 50 & 11.18 & 53.92 & 10.61 & 51.46 \\ \hline
                Small Water & 0.8 & 25.35 & 5.71 & 0.11 & 7.05 & 0.12 & 6.06 & 0.11 \\ \hline
                Rabies & 2414.14 & 25 & 1.65 & 264.14 & 2.05 & 281.85 & 1.67 & 273.08 \\ \hline
                Polysaccharide & 1886.41 & 25.17 & 11.35 & 245.19 & 11.88 & 264.65 & 11.35 & 253.29 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

%NN part 2

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=5;V=1 & & K=5;V=2 & & K=5;V=3 & \\ \hline
                HIV & 14.53 & 25.04 & 10.63 & 2.12 & 10.53 & 2.11 & 10.04 & 2.01 \\ \hline
                Water & 243.22 & 25 & 6.82 & 32.81 & 6.76 & 32.96 & 6.19 & 32.33 \\ \hline
                Hot Water & 221.11 & 25 & 8.34 & 30.97 & 8.07 & 30.87 & 7.5 & 28.67 \\ \hline
                MSCL & 381.15 & 25 & 11.3 & 57.52 & 11.17 & 56.45 & 10.66 & 53.01 \\ \hline
                Small Water & 0.8 & 25.35 & 7.36 & 0.13 & 7.19 & 0.13 & 6.4 & 0.12 \\ \hline
                Rabies & 2414.14 & 25 & 2.03 & 297.93 & 2.07 & 301.25 & 1.69 & 282.53 \\ \hline
                Polysaccharide & 1886.41 & 25.17 & 11.92 & 280.21 & 11.84 & 276.67 & 11.35 & 259.61 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

% NN part 3

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=10;V=1 & & K=10;V=2 & & K=10;V=3 & & K=10;V=4 & \\ \hline
                HIV & 14.53 & 25.04 & \textbf{10.99} & 3.33 & 11.2 & 3.25 & 11.31 & 3.46 & 11.23 & 3.59 \\ \hline
                Water & 243.22 & 25 & \textbf{7.16} & 48.03 & 7.33 & 52.19 & 7.5 & 56.8 & 7.49 & 59.67 \\ \hline
                Hot Water & 221.11 & 25 & 8.92 & 46.33 & 8.83 & 48.8 & 8.76 & 52.3 & \textbf{8.68} & 54.66 \\ \hline
                MSCL & 381.15 & 25 & \textbf{11.58} & 83.34 & 11.69 & 88.26 & 11.75 & 94.92 & 11.71 & 99.15 \\ \hline
                Small Water & 0.8 & 25.35 & 8.76 & 0.24 & \textbf{8.74} & 0.2 & 8.85 & 0.21 & 8.77 & 0.22 \\ \hline
                Rabies & 2414.14 & 25 & 2.19 & 417.72 & \textbf{2.12} & 481.66 & 2.18 & 529.74 & 2.19 & 557.83 \\ \hline
                Polysaccharide & 1886.41 & 25.17 & \textbf{12} & 373.49 & 12.1 & 428.81 & 12.19 & 458.1 & 12.19 & 476.76 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\section{12-bit Quantisation}
\subsection{Polynomial Extrapolation}
\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=1 & & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 37.53 & 20.78 & 2.52 & 22.59 & 2.83 & 25.17 & 2.96 & 30.84 & 3.59 & 41.19 & 5.87 \\ \hline
                Water& 243.22 & 37.5 & 16.36 & 41.33 & 18.14 & 41.61 & 20.66 & 45.34 & 26.19 & 53.06 & 38.83 & 86.32 \\ \hline
                Hot Water& 221.11 & 37.5 & 17.64 & 38.09 & 19.2 & 38.33 & 21.75 & 41.39 & 27.32 & 48.78 & 39.23 & 79.33 \\ \hline
                MSCL& 381.15 & 37.5 & 21.85 & 71.53 & 23.99 & 71.9 & 26.6 & 78.31 & 32.07 & 93.43 & 40.31 & 140.06 \\ \hline
                Small Water & 0.8 & 37.93 & 13.95 & 0.18 & 11.82 & 0.16 & 13.53 & 0.19 & 20.63 & 0.24 & 39.76 & 0.41 \\ \hline
                Rabies & 2414.14 & 37.5 & 7.55 & 336.29 & 8.93 & 366.28 & 11.46 & 404.61 & 17.18 & 480.64 & 31.56 & 703.4 \\ \hline
                Polysaccharide & 1886.41 & 37.65 & 23.03 & 341.42 & 25.34 & 365.56 & 27.98 & 401.43 & 33.37 & 497.1 & 40.62 & 734.52 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

%SPLINE
\subsection{Spline Extrapolation}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 37.53 & 23.35 & 2.89 & 24.22 & 3.26 & 25.54 & 4.14 \\ \hline
                Water& 243.22 & 37.5 & 18.84 & 44.78 & 19.56 & 49 & 20.5 & 61.42 \\ \hline
                Hot Water& 221.11 & 37.5 & 19.92 & 40.81 & 20.67 & 45.12 & 21.66 & 55.37 \\ \hline
                MSCL& 381.15 & 37.5 & 24.79 & 77.71 & 25.5 & 85.12 & 26.18 & 103.25 \\ \hline
                Small Water & 0.8 & 37.93 & 12.63 & 0.18 & 14.74 & 0.23 & 19.85 & 0.35 \\ \hline
                Rabies & 2414.14 & 37.5 & 9.56 & 389.75 & 10.32 & 438.21 & 11.01 & 553.1 \\ \hline
                Polysaccharide & 1886.41 & 37.65 & 26.12 & 395.72 & 26.76 & 431.96 & 27.25 & 532.68 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\newpage
\subsection{Smallest Error Encoding}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
		Dataset & Original & Quantised & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 37.53 & 23.06 & 5.85 & 24.12 & 7.37 & 25.56 & 9.16 & 27.83 & 11.6 \\ \hline
                Water& 243.22 & 37.5 & 18.52 & 93.52 & 19.55 & 117.92 & 20.89 & 147.12 & 22.69 & 188.42 \\ \hline
                Hot Water& 221.11 & 37.5 & 19.88 & 85.83 & 20.99 & 108.32 & 22.43 & 135.01 & 24.54 & 173.24 \\ \hline
                MSCL& 381.15 & 37.5 & 23.64 & 152.9 & 24.55 & 192.25 & 25.69 & 237.67 & 27.27 & 302.72 \\ \hline
                Small Water & 0.8 & 37.93 & 17.9 & 0.34 & 19.74 & 0.44 & 22.76 & 0.55 & 29.15 & 0.73 \\ \hline
                Rabies & 2414.14 & 37.5 & 9.51 & 850.09 & 10.8 & 1083.01 & 12.24 & 1372.13 & 14.32 & 1794.95 \\ \hline
                Polysaccharide & 1886.41 & 37.65 & 24.53 & 764.27 & 25.29 & 899.42 & 26.22 & 1169.6 & 27.44 & 1463.71 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\subsection{Common Error Encoding}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=2 & & K=3 &  & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 37.53 & 21.55 & 6.22 & 22.11 & 7.09 & 23.06 & 10.05 & 24.77 & 16.11 \\ \hline
                Water& 243.22 & 37.5 & 17.02 & 84.98 & 17.5 & 105.25 & 18.27 & 148.97 & 19.42 & 241.21 \\ \hline
                Hot Water& 221.11 & 37.5 & 18.51 & 80.44 & 19.26 & 102.02 & 20.55 & 137.74 & 22.74 & 218.11 \\ \hline
                MSCL& 381.15 & 37.5 & 22.19 & 158.78 & 22.56 & 193.28 & 23.21 & 271.8 & 24.4 & 423.97 \\ \hline
                Small Water & 0.8 & 37.93 & 16.43 & 0.3 & 18.29 & 0.39 & 21.26 & 0.54 & 27.37 & 0.89 \\ \hline
                Rabies & 2414.14 & 37.5 & 8.33 & 709.29 & 9.02 & 881.26 & 9.83 & 1213.44 & 11.25 & 2017.65 \\ \hline
                Polysaccharide & 1886.41 & 37.65 & 23.12 & 799.51 & 23.31 & 1029.08 & 23.68 & 1408.46 & 24.36 & 2298.6 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\subsection{k-Nearest Neighbour Encoding}
\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3;V=1 & & K=4;V=1 &  & K=4;V=2 &  \\ \hline
                HIV & 14.53 & 37.53 & 22.6 & 2.53 & 23.22 & 2.74 & 22.75 & 2.65 \\ \hline
                Water& 243.22 & 37.5 & 17.96 & 37.33 & 18.55 & 40.01 & 18.06 & 39.2 \\ \hline
                Hot Water& 221.11 & 37.5 & 19.49 & 35.08 & 20.11 & 38.01 & 19.61 & 36.01 \\ \hline
                MSCL& 381.15 & 37.5 & 23 & 65.44 & 23.59 & 69.74 & 23.05 & 67.09 \\ \hline
                Small Water & 0.8 & 37.93 & 18.78 & 0.18 & 19.87 & 0.2 & 19.71 & 0.2 \\ \hline
                Rabies & 2414.14 & 37.5 & 9.29 & 326.96 & 10.15 & 350.45 & 9.32 & 335.28 \\ \hline
                Polysaccharide & 1886.41 & 37.65 & 23.78 & 332.38 & 24.3 & 351.75 & 23.78 & 340.34 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

%NN part 2

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=5;V=1 & & K=5;V=2 & & K=5;V=3 &  \\ \hline
                HIV & 14.53 & 37.53 & 23.44 & 2.91 & 23.35 & 2.85 & 22.91 & 2.72 \\ \hline
                Water& 243.22 & 37.5 & 18.73 & 43.08 & 18.63 & 41.85 & 18.16 & 39.4 \\ \hline
                Hot Water& 221.11 & 37.5 & 20.32 & 39.4 & 20.11 & 39.02 & 19.71 & 36.88 \\ \hline
                MSCL& 381.15 & 37.5 & 23.72 & 73.17 & 23.59 & 75.47 & 23.1 & 68.8 \\ \hline
                Small Water & 0.8 & 37.93 & 20.86 & 0.23 & 20.78 & 0.22 & 20.62 & 0.22 \\ \hline
                Rabies & 2414.14 & 37.5 & 10.14 & 373.29 & 9.93 & 368.05 & 9.34 & 343.49 \\ \hline
                Polysaccharide & 1886.41 & 37.65 & 24.36 & 368.35 & 24.25 & 365.49 & 23.78 & 347.8 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

% NN part 3

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=10;V=1 & & K=10;V=2 & & K=10;V=3 & & K=10;V=4 &  \\ \hline
                HIV & 14.53 & 37.53 & 24.28 & 4.27 & 24.52 & 4.19 & 24.63 & 4.42 & 24.53 & 4.56 \\ \hline
                Water& 243.22 & 37.5 & 19.11 & 59.81 & 19.42 & 62.33 & 19.57 & 66.36 & 19.46 & 68.9 \\ \hline
                Hot Water& 221.11 & 37.5 & 20.9 & 57.76 & 20.95 & 58.42 & 20.95 & 60.98 & 20.86 & 64.15 \\ \hline
                MSCL& 381.15 & 37.5 & 24.01 & 99.88 & 24.16 & 104.61 & 24.23 & 110.76 & 24.18 & 114.69 \\ \hline
                Small Water & 0.8 & 37.93 & 25.29 & 0.4 & 25.63 & 0.37 & 25.62 & 0.39 & 25.4 & 0.39 \\ \hline
                Rabies & 2414.14 & 37.5 & 10.14 & 511.6 & 10.17 & 574.95 & 10.15 & 614.86 & 10.02 & 643.33 \\ \hline
                Polysaccharide & 1886.41 & 37.65 & 24.4 & 471.41 & 24.55 & 516.58 & 24.63 & 548 & 24.62 & 565.54 \\ \hline
        \end{tabular}}
\end{center}
\end{table}


\section{16-bit Quantisation}
\subsection{Polynomial Extrapolation}
\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=1 & & K=2 &  & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 50.02 & 33.63 & 3.49 & 35.71 & 3.88 & 38.62 & 4.66 & 45.18 & 6.33 & 58.84 & 10.49 \\ \hline
                Water & 243.22 & 50 & 28.93 & 52.59 & 30.78 & 52.01 & 33.38 & 58.34 & 39.08 & 78.77 & 52.27 & 132.22 \\ \hline
                Hot Water& 221.11 & 50 & 30.22 & 48.04 & 31.86 & 48.71 & 34.5 & 55.39 & 40.26 & 74.62 & 52.76 & 122.31 \\ \hline
                MSCL & 381.15 & 50 & 34.42 & 96.42 & 36.62 & 102.63 & 39.29 & 119.44 & 44.89 & 151.89 & 53.43 & 216.15 \\ \hline
                Small Water& 0.8 & 50.43 & 26.76 & 0.24 & 24.26 & 0.2 & 25.44 & 0.23 & 32.72 & 0.3 & 58.77 & 0.71 \\ \hline
                Rabies& 2414.14 & 50 & 19.69 & 419.97 & 20.81 & 444.13 & 23.32 & 480.77 & 29.15 & 569.31 & 44.31 & 1008.73 \\ \hline
                Polysaccharide& 1886.41 & 50.13 & 35.52 & 489.06 & 37.83 & 560.47 & 40.49 & 644.77 & 45.87 & 783.74 & 53.12 & 1083.71 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

%SPLINE
\subsection{Spline Extrapolation}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3 & & K=5 &  & K=10 & \\ \hline
                HIV & 14.53 & 50.02 & 36.73 & 4.34 & 38.08 & 5.01 & 40.59 & 6.7 \\ \hline
                Water & 243.22 & 50 & 31.56 & 56.05 & 32.44 & 63 & 33.75 & 80.23 \\ \hline
                Hot Water& 221.11 & 50 & 32.67 & 52.87 & 33.59 & 59.15 & 34.99 & 75.26 \\ \hline
                MSCL & 381.15 & 50 & 37.49 & 112.34 & 38.3 & 124.82 & 39.23 & 149.11 \\ \hline
                Small Water& 0.8 & 50.43 & 24.88 & 0.23 & 26.94 & 0.28 & 32.32 & 0.42 \\ \hline
                Rabies & 2414.14 & 50 & 21.46 & 468.82 & 22.15 & 518.14 & 22.86 & 644.05 \\ \hline
                Polysaccharide& 1886.41 & 50.13 & 38.63 & 608.04 & 39.27 & 649.04 & 39.76 & 752.78 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\newpage
\subsection{Smallest Error Encoding}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
		Dataset & Original & Quantised & K=2 &  & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 50.02 & 36.14 & 6.99 & 37.4 & 8.3 & 39.3 & 10.32 & 42.74 & 12.64 \\ \hline
                Water & 243.22 & 50 & 31.18 & 106.73 & 32.24 & 130.72 & 33.72 & 165.6 & 35.9 & 207 \\ \hline
                Hot Water& 221.11 & 50 & 32.57 & 97.89 & 33.72 & 121.12 & 35.31 & 150.06 & 37.83 & 193.71 \\ \hline
                MSCL & 381.15 & 50 & 36.26 & 177.52 & 37.21 & 216.9 & 38.44 & 259.88 & 40.26 & 325.1 \\ \hline
                Small Water& 0.8 & 50.43 & 30.79 & 0.38 & 32.56 & 0.48 & 35.72 & 0.62 & 42.37 & 0.81 \\ \hline
                Rabies& 2414.14 & 50 & 21.98 & 965.71 & 23.01 & 1212.31 & 24.22 & 1522.3 & 26 & 1959.87 \\ \hline
                Polysaccharide& 1886.41 & 50.13 & 37.03 & 894.1 & 37.78 & 1071.02 & 38.71 & 1313.99 & 39.92 & 1550.5 \\ \hline

        \end{tabular}}
\end{center}
\end{table}

\subsection{Common Error Encoding}

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=2 & & K=3 & & K=5 & & K=10 & \\ \hline
                HIV & 14.53 & 50.02 & 34.77 & 10.45 & 35.58 & 13.16 & 37.04 & 17.85 & 39.98 & 27.32 \\ \hline
                Water & 243.22 & 50 & 29.67 & 128.05 & 30.23 & 159.52 & 31.15 & 226.87 & 32.69 & 352.83 \\ \hline
                Hot Water& 221.11 & 50 & 31.18 & 126.2 & 32.02 & 161.41 & 33.48 & 232.67 & 36.1 & 389.37 \\ \hline
                MSCL & 381.15 & 50 & 34.83 & 287.16 & 35.25 & 349.55 & 36 & 485.93 & 37.45 & 729.98 \\ \hline
                Small Water& 0.8 & 50.43 & 29.75 & 0.45 & 31.86 & 0.56 & 35.28 & 0.83 & 42.28 & 1.33 \\ \hline
                Rabies& 2414.14 & 50 & 20.57 & 947.12 & 21.31 & 1174.86 & 22.19 & 1610.66 & 23.7 & 2597.7 \\ \hline
                Polysaccharide& 1886.41 & 50.13 & 35.63 & 1560.1 & 35.82 & 1900.33 & 36.19 & 2549.82 & 36.87 & 3901.6 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\subsection{k-Nearest Neighbour Encoding}
\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=3;V=1 & & K=4;V=1 & & K=4;V=2 &  \\ \hline
                HIV & 14.53 & 50.02 & 35.96 & 3.91 & 36.81 & 4.27 & 36.33 & 4.09 \\ \hline
                Water & 243.22 & 50 & 30.68 & 49.52 & 31.34 & 53.23 & 30.86 & 51.33 \\ \hline
                Hot Water& 221.11 & 50 & 32.25 & 47.68 & 32.93 & 50.75 & 32.44 & 48.73 \\ \hline
                MSCL & 381.15 & 50 & 35.67 & 94.51 & 36.31 & 99.82 & 35.77 & 95.28 \\ \hline
                Small Water& 0.8 & 50.43 & 31.96 & 0.23 & 33.08 & 0.26 & 32.95 & 0.26 \\ \hline
                Rabies& 2414.14 & 50 & 21.63 & 416.88 & 22.29 & 443.53 & 21.67 & 426.21 \\ \hline
                Polysaccharide& 1886.41 & 50.13 & 36.28 & 500.59 & 36.79 & 529.29 & 36.28 & 509.81 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

%NN part 2

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=5;V=1 & & K=5;V=2 &  & K=5;V=3 &  \\ \hline
                HIV & 14.53 & 50.02 & 37.27 & 4.55 & 37.18 & 4.51 & 36.73 & 4.33 \\ \hline
                Water & 243.22 & 50 & 31.6 & 56.22 & 31.5 & 56.83 & 31.04 & 53.79 \\ \hline
                Hot Water& 221.11 & 50 & 33.22 & 53.7 & 33.03 & 53.01 & 32.64 & 50.46 \\ \hline
                MSCL & 381.15 & 50 & 36.49 & 104.8 & 36.36 & 103.39 & 35.87 & 98.24 \\ \hline
                Small Water& 0.8 & 50.43 & 34.16 & 0.29 & 34.09 & 0.29 & 33.93 & 0.28 \\ \hline
                Rabies& 2414.14 & 50 & 22.34 & 465.25 & 22.14 & 459.57 & 21.7 & 437.7 \\ \hline
                Polysaccharide& 1886.41 & 50.13 & 36.85 & 566.16 & 36.75 & 545.91 & 36.28 & 523.22 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

% NN part 3

\begin{table}[!h]
\begin{center}
\resizebox{\textwidth}{!}{
        \begin{tabular}{ | l | l | l | ll | ll | ll | ll |}
                \hline
                Dataset & Original & Quantised & K=10;V=1 & & K=10;V=2 & & K=10;V=3 & & K=10;V=4 &  \\ \hline
                HIV & 14.53 & 50.02 & 39.3 & 6.38 & 39.54 & 6.7 & 39.64 & 6.9 & 39.54 & 7 \\ \hline
                Water & 243.22 & 50 & 32.39 & 78.07 & 32.68 & 80.52 & 32.83 & 84.52 & 32.72 & 86.9 \\ \hline
                Hot Water& 221.11 & 50 & 34.24 & 75.05 & 34.29 & 75.92 & 34.29 & 79.47 & 34.2 & 81.49 \\ \hline
                MSCL & 381.15 & 50 & 37.02 & 136.19 & 37.18 & 141.06 & 37.24 & 147.71 & 37.19 & 153.03 \\ \hline
                Small Water& 0.8 & 50.43 & 38.96 & 0.51 & 39.32 & 0.45 & 39.35 & 0.47 & 39.08 & 0.47 \\ \hline
                Rabies& 2414.14 & 50 & 22.43 & 609.8 & 22.48 & 666.01 & 22.45 & 703.26 & 22.33 & 732.68 \\ \hline
                Polysaccharide& 1886.41 & 50.13 & 36.9 & 662.25 & 37.05 & 715.88 & 37.13 & 754.73 & 37.12 & 760.93 \\ \hline
        \end{tabular}}
\end{center}
\end{table}

\end{document}
