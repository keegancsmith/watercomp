\documentclass{report}

\usepackage{natbib}
\usepackage{graphicx}
%\usepackage{pslatex}
%\usepackage[margin=2.25cm]{geometry}
\usepackage{multirow}
\bibliographystyle{abbrvnat}

\title{Literature Review of Point Cloud Compression}
\author{Keegan Smith\\ksmith@cs.uct.ac.za}

\begin{document}

\begin{titlepage}
\begin{center}
\includegraphics[width=100mm]{images/uct}\\
\ \\
\textsc{\Large
Department of Computer Science\\
\ \\
Honours Project Report\\
\ \\}
%
% the title
%
{\huge \bfseries
TODO Insert title here!
\\}
\ \\
\ \\

\begin{center}
%  \begin{tabular}{lll}
    \large Keegan Carruthers-Smith% & \large Julian Kenwood & \large Min-Young Wu
    \\
    \small{ksmith@cs.uct.ac.za}% & \small{jkenwood@cs.uct.ac.za} & \small{mwu@cs.uct.ac.za} \\
%  \end{tabular}
\end{center}

\vfill % fill vertical space
%
% Bottom of the page
% \today is the compilation date
{\large \today}
%
\end{center}
\end{titlepage}


\section*{Abstract}
Molecular dynamics (MD) simulations generate vast amounts of data. A typical
100-million atom MD simulation produces approximately 5 gigabytes of data per
frame consisting of atom types, coordinates and velocities.

This report surveys techniques used in point cloud compression. A point cloud
is a collection of points in 3D space. It then investigates the structure of
water. We then present a method for compressing MD data using connectivity
based point cloud compression with predictors tailored towards the structure
of water.

\textbf{TODO} Talk about results etc once they have been produced!

\tableofcontents

\chapter{Introduction}

Molecular dynamics (MD) simulations generate vast amounts of data. A typical
100-million atom MD simulation produces approximately 5 gigabytes of data per
frame consisting of atom types, coordinates and velocities. This will generate
17 terabytes of data a day if run for $35\,000$ steps with every 10th frame
being saved \cite{omeltchenko2000sls}. Generating this much data makes good
compression very desirable.

In this paper we will discuss current methods used to compress point
clouds. Specifically, we want to compress point clouds of water molecules.

Firstly, we will discuss common techniques used in point cloud compression
algorithms, and then move onto different categorizations and how they are
applied. We will then discuss how to apply these techniques to water molecule
compression. Finally, we will conclude with two general categorisations of
what the compression algorithms exploit.

\chapter{Background}

\section{Point Cloud Compression}

Most research on point clouds involves point clouds of surfaces. This is
because most point clouds are obtained from 3D scans of object surfaces. A
surface is the ``skin'' of an object, i.e. the boundaries of a solid object.

Gumhold et al.~\cite{gumholdcomp} create a tree of the points that exploits
the knowledge that the points lie on a surface. They do not, however, use
predictors which exploit this. Merry et al.~\cite{merrycomp} extend on this by
using predictors which do exploit the rectilinear nature of surface scans.

Omeltchenko et al.~\cite{omeltchenko2000sls} compress point clouds from
molecular dynamics simulations. To exploit the density of the point clouds, a
space filling curve\footnote{A space filling curve is one that touches every
  point in the space. Omeltchenko et al.~\cite{omeltchenko2000sls} create this
  using a techniques similiar to an octree.} is created along the quantized
positions. So, every point is assigned an index along the curve. Then by
encoding the difference between successive indicies which contain points, they
get a lot of common small differences leading to good compression rates.

Devillers and Gandoin \cite{devillers2000gci} compress meshes of 3D
models. The method they use only considers point locations, so it can be used
for point cloud compression. Their method exploits a property of kd-trees,
instead of general geometric properties. A mesh is a collection of vertices,
edges and faces defining a 3D object.


\section{Quantization}

Point cloud data usually stores the point coordinates in a floating point
format. Most geometric compression algorithms reduce the precision to achieve
better compression rates. This reduction of precision is called
\emph{quantization} \cite{ag-racm-03}. Every algorithm reviewed in this paper
that uses quantization, quantizes the vertex positions for each coordinate
separately and uniformly in Cartesian space. This can be visualized as
creating an evenly-spaced grid over the dimensions of the space, and snapping
the point to the closest grid point. This is the most na\"{\i}ve method, but
there are more complex methods reviewed by Pierre Alliez and Craig
Gotsman\cite{ag-racm-03}. These lie outside the scope of this survey. Chen et
al.~\cite{chen2005lcp} get around quantization by treating the 32-bit floating
point numbers as 32-bit integers.


\section{Predictive Compression}

Predictive compressors try to predict where a point is and then encode the
difference between its actual location and the predicted location (known as
the residual). These residuals are then encoded such that the decompressor can
calculate the point's location by the prediction added to the
residual. Entropy coding\footnote{Entropy encoding is a lossless compression
  scheme that does not exploit any characteristics of the underlying data.} is
usually applied to the encoding of residuals. If the chosen predictor(s)
exploit contextual information well, the residuals should have a low
entropy\footnote{A low entropy means we can predict what a residual will be
  well.}  leading to higher compression ratios.


\subsection{Predictors}

Both papers discussed depend on a spanning tree of the points. This will be
explained in the next subsection, but for now let $v'$ be the parent of the
point $v$, and $v''$ be the point's grandparent in the tree.

Gumhold et al.~\cite{gumholdcomp} lets the user choose between two na\"{\i}ve
predictors. The first is a \emph{constant} predictor where $v$ is predicted to
be at $v'$. The second is a \emph{linear} predictor where $v$ is predicted to
be at $v' + (v' - v'')$. So either $v$ is in the same place as its parent, or
it is along the straight line $(v', v'')$ with distance $|v'-v''|$ away from
its parent.

Merry et al.~\cite{merrycomp} use the same predictors as Gumhold et al.\ but
with two additional predictors, the \emph{left} and \emph{right}
predictors. The surface normal\footnote{A surface normal at a point $v$ is the
  normal of the plane tangent to the surface at $v$.} $n_v$ at $v$ is
heuristically determined from $v, v'', v'$ and $n_{v'}$ (where $n_{v'}$ is the
surface normal at $v'$). From this they can determine what left and right are
by using the plane determined by $n_{v'}, v'$ and $v''$. Another difference to
Gumhold et al.\ is that instead of letting the user choose the predictor used,
the best predictor is picked for each point. This requires the predictor used
at each point to be encoded, which is discussed next.


\subsection{Serialization}

Unlike meshes, point clouds have no connectivity information. So there is no
obvious order to serialize the vertices' information (generally residuals).

Gumhold et al.~\cite{gumholdcomp} create a rooted spanning tree of the
points. The tree is greedily created by randomnly picking a root and adding
each successive point to the vertex which minimises the residual. The tree is
then serialized by entropy encoding the out degree of each vertex in
breadth-first order.

Merry et al.~\cite{merrycomp} also create a rooted spanning tree of the
points, but in a way that favours ``long runs'' of the forward predictor. They
need to encode the predictor used at each point, so encoding the tree as
Gumhold et al.\ did will not work. Instead the tree is serialized by entropy
coding the predictor used in depth-first order. By favouring long runs of the
forward predictor they get better compression ratios of the encoding of the
tree. To create the spanning tree, they first create a graph of the points
where every edge with length less than $L$ is added. $L$ is the length of the
longest edge in the minimum spanning tree of the complete graph of the
points. The spanning tree is then created similarly to Prim's algorithm
\cite[p.\ 457]{sedgewick}, but instead using a metric\footnote{A metric is a
  way to calculate the distance between vertices.} that favours recently
considered vertices and ones predicted well by the forward predictor.

Chen et al.\cite{chen2005lcp} uses a spanning tree with differential coding on
the weights. So the point cloud is encoded as a tree, with edge weights being
the difference vectors between points. They show that there exists a spanning
tree which minimises the number of pairwise different edge weights, but also
show that this is NP-Hard. They do, however, give an approximate algorithm
which uses clustering and minimum spanning trees. The tree is serialized in a
simple breadth-first order, recording the number of children for each node and
the edge weights for each child.


\section{Decoding}

When decoding the data, we may present to the user intermediate
representations of the final decompressed data as we process it. Depending on
whether we can do this or not gives us another way to classify encoders.

\subsection{Progressive Encoding}

A progressive encoder is one that starts out streaming a coarse representation
of a point cloud. It then streams out refinements. This is useful for
transmitting the data over a network, as users can almost instantaneously see
a coarse representation. This is then followed by more and more refinements
over time as they arrive.

Devillers and Gandoin \cite{devillers2000gci} provide an example of a
progressive coder. The method is best explained in 1 dimension first. If you
know the total number of points in $[a, b]$ (say $x$ points) and the number of
points in $[a, (a+b)/2]$ (say $y$ points) then the number of points in
$[(a+b)/2+1, b]$ is $x - y$ . By sending out counts of deeper and deeper
levels in breadth first order they progressively get a better
representation. Encoding this with arithmetic coding gives us the desired
compression ratios. To extend to 3 dimensions one uses kd-trees and at each
step subdivide along a different dimension.


\subsection{Single-Rate Encoding}

A single-rate encoder is one which requires the whole stream to be available
before the data can be decompressed. Omeltchenko et
al.~\cite{omeltchenko2000sls} Gumhold et al.~\cite{gumholdcomp} and Merry et
al.~\cite{merrycomp} are all single-rate encoders.


\section{Water Heuristics}

\textbf{TODO}
%% Point clouds of water molecules are a collection of hydrogen and oxygen atoms
%% (two hydrogens for every oxygen) and their locations. Due to the way the
%% molecules interact, we have very good knowledge on how the molecules will be
%% laid out relatively to each other.

%% Omeltchenko et al.'s~\cite{omeltchenko2000sls} method only exploits the fact
%% that we have many dense clusters in MD simulations. It is more general than
%% water compression, and modifying it to use the relative layouts of the water
%% molecules is not clear.

%% Merry et al.'s~\cite{merrycomp} and Gumhold et al.'s~\cite{gumholdcomp}
%% predictors do not exploit the layout of water molecules, but using predictors
%% is a very salient idea. Using the knowledge we have about water molecules, we
%% can predict where a water molecule will occur relative to another one. The
%% spanning tree would be different, but similiar ideas will carry over.

%% Single-rate encoders usually have higher compression ratios than progressive
%% encoders. This survey covered progressive coders, but these are not as
%% applicable to MD compression since the application of the MD data requires the
%% whole file. So we will not discuss them further.


\section{Conclusion}

Refer to Figure \ref{fig:taxonomy} for an overview of the categorisations of
the different papers. There are two underlying philosophies which the
compression algorithms exploit, the \emph{scope} and the \emph{topology}.

The \emph{scope} is whether the algorithm is exploiting \emph{global} or
\emph{local} properties of the points. Merry et al.~\cite{merrycomp} uses a
local scope since it exploits nearby points to predict the location of the
current point. Chen et al.~\cite{chen2005lcp} uses a global scope, since it is
trying to minimise a global property of the spanning tree.

The \emph{topology} is whether the algorithm is trying to exploit connectivity
or geometric information. Gumhold et al.~\cite{gumholdcomp} exploits both, but
the geometry slightly more since it uses a predictor to predict the location
based on other points location. Chen et al.~\cite{chen2005lcp} just tries to
choose the best possible spanning tree, so they exploit connectivity of
points.

For the compression of water molecules we have a lot of local heuristics. From
these we have good geometric and connectivity information. So the best
matching approach would come from something similiar to the predictive
single-rate compression algorithms of Merry et al.~\cite{merrycomp} and
Gumhold et al.\cite{gumholdcomp}.

\begin{figure*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{||l|c|c||}
  \hline
  \emph{Attribute/Method} & \cite{omeltchenko2000sls} & \cite{gumholdcomp} \\

  \hline

  \emph{Structure} & MD Simulation & Surface \\

  \emph{Progressive} & Single-rate & Single-rate \\

  \emph{Quantization} & Yes & Yes \\

  \emph{Connectivity} & Space Filling Curve & Spanning Tree \\

  \emph{Exploits} & Dense/regular clusters & Regularity of scans \\

  \emph{Scope} & Global & Local \\

  \emph{Topology} & Geometry & Both \\

  \hline
  \hline

  \emph{Attribute/Method} & \cite{chen2005lcp} & \cite{devillers2000gci} \\

  \hline

  \emph{Structure} & 3D Model & Mesh \\

  \emph{Progressive} & Single-rate & Progressive \\

  \emph{Quantization} & No & Yes \\

  \emph{Connectivity} & Spanning Tree & kd-trees \\

  \emph{Exploits} & Differential Coding & kd-tree properties \\

  \emph{Scope} & Global & Global \\

  \emph{Topology} & Connectivity & Neither \\

  \hline
  \hline

  \emph{Attribute/Method} & \cite{merrycomp} & \\

  \hline

  \emph{Structure} & Surface & \\

  \emph{Progressive} & Single-rate & \\  

  \emph{Quantization} & Yes & \\

  \emph{Connectivity} & Spanning Tree & \\

  \emph{Exploits} & Rectilinear scans & \\

  \emph{Scope} & Local & \\

  \emph{Topology} & Both & \\

  \hline
\end{tabular}
}
\caption{Taxonomy of the different methods.}\label{fig:taxonomy}
\end{figure*}


\nocite{*}
\bibliography{report_keegan}
\bibliographystyle{acm}

\end{document}
