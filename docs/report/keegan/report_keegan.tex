\documentclass{report}


\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage[T1]{fontenc}
\usepackage{pxfonts}
\usepackage{graphicx}
%\usepackage{pslatex}
%\usepackage[margin=2.25cm]{geometry}
\usepackage{multirow}

\usepackage{natbib}
\bibliographystyle{abbrvnat}

\title{Literature Review of Point Cloud Compression}
\author{Keegan Smith\\ksmith@cs.uct.ac.za}

\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[width=100mm]{images/uct}\\
\ \\
\textsc{\Large
Department of Computer Science\\
\ \\
Honours Project Report\\
\ \\}

{\huge \bfseries
TODO Insert title here!
\\}
\ \\
\ \\

\begin{center}
    \large Keegan Carruthers-Smith
    \\
    \small{ksmith@cs.uct.ac.za}
\end{center}

\vfill

{\large \today}

\end{center}
\end{titlepage}


\section*{Abstract}
Molecular dynamics (MD) simulations generate vast amounts of data. A typical
100-million atom MD simulation produces approximately 5 gigabytes of data per
frame consisting of atom types, coordinates and velocities.

This report surveys techniques used in point cloud compression. A point cloud
is a collection of points in 3D space. It then investigates the structure of
water. We then present a method for compressing MD data using connectivity
based point cloud compression with predictors tailored towards the structure
of water.

\textbf{TODO} Talk about results etc once they have been produced!

\tableofcontents

\chapter{Introduction}

Molecular dynamics (MD) simulations generate vast amounts of data. A typical
100-million atom MD simulation produces approximately 5 gigabytes of data per
frame consisting of atom types, coordinates and velocities. This will generate
17 terabytes of data a day if run for $35\,000$ steps with every 10th frame
being saved \citep{omeltchenko2000sls}. Generating this much data makes good
compression very desirable.

We have developed a method which compresses MD simulations. It targets
simulations which have a large amount of water in them. It uses connectivity
based point cloud techniques, but with predictors which based on the structure
of water.

\textbf{TODO} Flesh out introduction

\chapter{Background}

We will discuss common techniques used in point cloud compression algorithms,
and then move onto different categorizations and how they are applied. We will
then discuss how to apply these techniques to water molecule
compression. Finally, we will conclude with two general categorisations of
what the compression algorithms exploit.

\section{Point Cloud Compression}

Most research on point clouds involves point clouds of surfaces. This is
because most point clouds are obtained from 3D scans of object surfaces. A
surface is the ``skin'' of an object, i.e. the boundaries of a solid object.

\citet{gumholdcomp} create a tree of the points that exploits the knowledge
that the points lie on a surface. They do not, however, use predictors which
exploit this. \citet{merrycomp} extend on this by using predictors which do
exploit the rectilinear nature of surface scans.

\citet{omeltchenko2000sls} compress point clouds from molecular dynamics
simulations. To exploit the density of the point clouds, a space filling
curve\footnote{A space filling curve is one that touches every point in the
  space. \citet{omeltchenko2000sls} create this using a techniques similiar to
  an octree.} is created along the quantised positions. So, every point is
assigned an index along the curve. Then by encoding the difference between
successive indicies which contain points, they get a lot of common small
differences leading to good compression rates.

\citet{devillers2000gci} compress meshes of 3D models. The method they use
only considers point locations, so it can be used for point cloud
compression. Their method exploits a property of kd-trees, instead of general
geometric properties. A mesh is a collection of vertices, edges and faces
defining a 3D object.


\section{Quantisation}

Point cloud data usually stores the point coordinates in a floating point
format. Most geometric compression algorithms reduce the precision to achieve
better compression rates. This reduction of precision is called
\emph{quantisation} \citep{ag-racm-03}. Every algorithm reviewed in this paper
that uses quantisation, quantises the vertex positions for each coordinate
separately and uniformly in Cartesian space. This can be visualized as
creating an evenly-spaced grid over the dimensions of the space, and snapping
the point to the closest grid point. This is the most na\"{\i}ve method, but
there are more complex methods reviewed by \citet{ag-racm-03}. These lie
outside the scope of this survey. \citet{chen2005lcp} get around quantisation
by treating the 32-bit floating point numbers as 32-bit integers.


\section{Predictive Compression}

Predictive compressors try to predict where a point is and then encode the
difference between its actual location and the predicted location (known as
the residual). These residuals are then encoded such that the decompressor can
calculate the point's location by the prediction added to the
residual. Entropy coding\footnote{Entropy encoding is a lossless compression
  scheme that does not exploit any characteristics of the underlying data.} is
usually applied to the encoding of residuals. If the chosen predictor(s)
exploit contextual information well, the residuals should have a low
entropy\footnote{A low entropy means we can predict what a residual will be
  well.}  leading to higher compression ratios.


\subsection{Predictors}

Both papers discussed depend on a spanning tree of the points. This will be
explained in the next subsection, but for now let $v'$ be the parent of the
point $v$, and $v''$ be the point's grandparent in the tree.

\citet{gumholdcomp} lets the user choose between two na\"{\i}ve
predictors. The first is a \emph{constant} predictor where $v$ is predicted to
be at $v'$. The second is a \emph{linear} predictor where $v$ is predicted to
be at $v' + (v' - v'')$. So either $v$ is in the same place as its parent, or
it is along the straight line $(v', v'')$ with distance $|v'-v''|$ away from
its parent.

\citet{merrycomp} use the same predictors as \citeauthor{gumholdcomp} but with
two additional predictors, the \emph{left} and \emph{right} predictors. The
surface normal\footnote{A surface normal at a point $v$ is the normal of the
  plane tangent to the surface at $v$.} $n_v$ at $v$ is heuristically
determined from $v, v'', v'$ and $n_{v'}$ (where $n_{v'}$ is the surface
normal at $v'$). From this they can determine what left and right are by using
the plane determined by $n_{v'}, v'$ and $v''$. Another difference to
\citeauthor{gumholdcomp} is that instead of letting the user choose the
predictor used, the best predictor is picked for each point. This requires the
predictor used at each point to be encoded, which is discussed next.


\subsection{Serialization}

Unlike meshes, point clouds have no connectivity information. So there is no
obvious order to serialize the vertices' information (generally residuals).

\citet{gumholdcomp} create a rooted spanning tree of the points. The tree is
greedily created by randomnly picking a root and adding each successive point
to the vertex which minimises the residual. The tree is then serialized by
entropy encoding the out degree of each vertex in breadth-first order.

\citet{merrycomp} also create a rooted spanning tree of the points, but in a
way that favours ``long runs'' of the forward predictor. They need to encode
the predictor used at each point, so encoding the tree as Gumhold et al.\ did
will not work. Instead the tree is serialized by entropy coding the predictor
used in depth-first order. By favouring long runs of the forward predictor
they get better compression ratios of the encoding of the tree. To create the
spanning tree, they first create a graph of the points where every edge with
length less than $L$ is added. $L$ is the length of the longest edge in the
minimum spanning tree of the complete graph of the points. The spanning tree
is then created similarly to Prim's algorithm \citep[p.\ 457]{sedgewick}, but
instead using a metric\footnote{A metric is a way to calculate the distance
  between vertices.} that favours recently considered vertices and ones
predicted well by the forward predictor.

\citet{chen2005lcp} uses a spanning tree with differential coding on the
weights. So the point cloud is encoded as a tree, with edge weights being the
difference vectors between points. They show that there exists a spanning tree
which minimises the number of pairwise different edge weights, but also show
that this is NP-Hard. They do, however, give an approximate algorithm which
uses clustering and minimum spanning trees. The tree is serialized in a simple
breadth-first order, recording the number of children for each node and the
edge weights for each child.


\section{Decoding}

When decoding the data, we may present to the user intermediate
representations of the final decompressed data as we process it. Depending on
whether we can do this or not gives us another way to classify encoders.

\subsection{Progressive Encoding}

A progressive encoder is one that starts out streaming a coarse representation
of a point cloud. It then streams out refinements. This is useful for
transmitting the data over a network, as users can almost instantaneously see
a coarse representation. This is then followed by more and more refinements
over time as they arrive.

\citet{devillers2000gci} provide an example of a progressive coder. The method
is best explained in 1 dimension first. If you know the total number of points
in $[a, b]$ (say $x$ points) and the number of points in $[a, (a+b)/2]$ (say
$y$ points) then the number of points in $[(a+b)/2+1, b]$ is $x - y$ . By
sending out counts of deeper and deeper levels in breadth first order they
progressively get a better representation. Encoding this with arithmetic
coding gives us the desired compression ratios. To extend to 3 dimensions one
uses a process similiar to creating a kd-tree: at each step subdivide along a
different dimension.


\subsection{Single-Rate Encoding}

A single-rate encoder is one which requires the whole stream to be available
before the data can be decompressed. \citet{omeltchenko2000sls},
\citet{gumholdcomp} and \citet{merrycomp} are all single-rate encoders.


\section{Water Heuristics}

\textbf{TODO}
%% Point clouds of water molecules are a collection of hydrogen and oxygen atoms
%% (two hydrogens for every oxygen) and their locations. Due to the way the
%% molecules interact, we have very good knowledge on how the molecules will be
%% laid out relatively to each other.

%% Omeltchenko et al.'s~\cite{omeltchenko2000sls} method only exploits the fact
%% that we have many dense clusters in MD simulations. It is more general than
%% water compression, and modifying it to use the relative layouts of the water
%% molecules is not clear.

%% Merry et al.'s~\cite{merrycomp} and Gumhold et al.'s~\cite{gumholdcomp}
%% predictors do not exploit the layout of water molecules, but using predictors
%% is a very salient idea. Using the knowledge we have about water molecules, we
%% can predict where a water molecule will occur relative to another one. The
%% spanning tree would be different, but similiar ideas will carry over.

%% Single-rate encoders usually have higher compression ratios than progressive
%% encoders. This survey covered progressive coders, but these are not as
%% applicable to MD compression since the application of the MD data requires the
%% whole file. So we will not discuss them further.


\section{Summary}

Refer to Figure \ref{fig:taxonomy} for an overview of the categorisations of
the different papers. There are two underlying philosophies which the
compression algorithms exploit, the \emph{scope} and the \emph{topology}.

The \emph{scope} is whether the algorithm is exploiting \emph{global} or
\emph{local} properties of the points. citet{merrycomp} uses a local scope
since it exploits nearby points to predict the location of the current
point. \citet{chen2005lcp} uses a global scope, since it is trying to minimise
a global property of the spanning tree.

The \emph{topology} is whether the algorithm is trying to exploit connectivity
or geometric information. \citet{gumholdcomp} exploits both, but the geometry
slightly more since it uses a predictor to predict the location based on other
points location. \citet{chen2005lcp} just tries to choose the best possible
spanning tree, so they exploit connectivity of points.

For the compression of water molecules we have a lot of local heuristics. From
these we have good geometric and connectivity information. So the best
matching approach would come from something similiar to the predictive
single-rate compression algorithms of \citet{merrycomp} and
\citet{gumholdcomp}.

\begin{figure*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{||l|c|c||}
  \hline
  \emph{Attribute/Method} & \citet{omeltchenko2000sls} & \citet{gumholdcomp} \\

  \hline

  \emph{Structure} & MD Simulation & Surface \\

  \emph{Progressive} & Single-rate & Single-rate \\

  \emph{Quantisation} & Yes & Yes \\

  \emph{Connectivity} & Space Filling Curve & Spanning Tree \\

  \emph{Exploits} & Dense/regular clusters & Regularity of scans \\

  \emph{Scope} & Global & Local \\

  \emph{Topology} & Geometry & Both \\

  \hline
  \hline

  \emph{Attribute/Method} & \citet{chen2005lcp} & \citet{devillers2000gci} \\

  \hline

  \emph{Structure} & 3D Model & Mesh \\

  \emph{Progressive} & Single-rate & Progressive \\

  \emph{Quantisation} & No & Yes \\

  \emph{Connectivity} & Spanning Tree & kd-trees \\

  \emph{Exploits} & Differential Coding & kd-tree properties \\

  \emph{Scope} & Global & Global \\

  \emph{Topology} & Connectivity & Neither \\

  \hline
  \hline

  \emph{Attribute/Method} & \citet{merrycomp} & \\

  \hline

  \emph{Structure} & Surface & \\

  \emph{Progressive} & Single-rate & \\  

  \emph{Quantisation} & Yes & \\

  \emph{Connectivity} & Spanning Tree & \\

  \emph{Exploits} & Rectilinear scans & \\

  \emph{Scope} & Local & \\

  \emph{Topology} & Both & \\

  \hline
\end{tabular}
}
\caption{Taxonomy of the different methods.}\label{fig:taxonomy}
\end{figure*}


\chapter{Design}

This section outlines the design for our compressor and decompressor. The
compression techniques I investigate only cover single frame compression, but
our final product handles multiframe compression and visualisation as well. As
such the design needs to take into account these uses, but I will only cover
the design aspects relevant to single frame compression.

I worked closely with Julian Kenwood on the compressor and decompressor, and
areas designated to him are more thoroughly explained in his report. I will
indicate such areas.

\section{Overview}

We target the PDB format, which is a format for storing MD simulations. PDB is
readable by VMD. \textbf{TODO} improve and references.

A high-level overview of how our compressor works is illustrated by the
following: \textbf{TODO} replace with pretty picture

\[ DCDFile \to Atoms \to QuantisedAtoms \to Graph \to SpanningTree \to SymbolStream \to
BitStream \]

The decompressor works in the reverse direction

\[ BitStream \to SymbolStream \to SpanningTree \to QuantisedAtoms \to
DCDFile \]

The decompressed DCD files have been quantised. Excluding the quantisation
step, our scheme is lossless. Notice that each step in the compression phase
has auxillary information. The auxillary information is required to
reconstruct reverse the process.


\section{Components}

We took a component-based approach to the design of the compressor. This is
because our algorithm naturally breaks down into different components. Our
reference implementations of other algorithms also make use of these
components.

The different components are the following:
\begin{itemize}
\item Compressors
\item Simulation I/O
\item Core Transformations
\item Arithmetic Coders
\item Miscellaneous Components
\item Verification Components
\end{itemize}

\subsection{Compressors}

The compressor component is the driver program. It utilises the other
components listed below to do compression and decompression. There is our
compressor and two reference compressors. The reference compressors are the
\citet{omeltchenko2000sls} scheme and the \citet{devillers2000gci} scheme.


\subsection{Simulation I/O}

These components handle I/O to and from the files we are compressing. We are
compressing DCD files. DCD files come with accompanying PDB file, which
contains info about what each atom is in the DCD file.

\textbf{TODO} Maybe list this info at a higher level.

\paragraph{DCDReader} (DCDFile file, int n) $\to$ Frame \\
Returns the $n^{th}$ frame of the DCDFile \emph{file}.

\paragraph{DCDWriter} (DCDFile file, int n, Frame frame) \\
Writes \emph{frame} as the $n^{th}$ frame in the DCDFile \emph{file}.

\paragraph{PDBReader} (String pdb\_path) $\to$ AtomInformation* \\
Returns a list of atom information in the file \emph{pdb\_path}.


\subsection{Core Transformation}

These components implement the core algorithms for each different
compressor. There are components for each reference implementation and our own
compressor.

\begin{verbatim}
Transform(Atoms *atoms, ArithmeticEncoder output):
  Atoms *water_atoms, *other_atoms = FindWater(atoms)
  Graph water_graph = CreateGraph(water_atoms)
  Tree spanning_tree = CreateSpanningTree(water_graph)
  TreeSerialise(output, tree)
\end{verbatim}

\textbf{TODO} detail the steps

\section{Arithmetic Coder}

The ``meat'' of all the compression schemes convert the input into a symbol
stream (where the symbols are from a fixed alphabet). These symbols are then
compressed with an arithmetic encoder. Similarly the decompressor takes a
symbol stream and converts it into the original input.

The arithmetic coder has two parts to it, a \emph{reader} and a
\emph{writer}. The \emph{reader} and the \emph{writer} are inverses to each
other, with the \emph{reader} converting a symbol stream to a bit stream and
the \emph{writer} converting a bit stream to a symbol stream.

The scheme by \citet{omeltchenko2000sls} does not make use of an Arithmetic
Coder, instead it uses its own coder. The implementation of the coders is
handled by Julian.


\nocite{*}
\bibliography{report_keegan}

\end{document}
