\documentclass{report}


\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage[T1]{fontenc}
\usepackage{pxfonts}
\usepackage{graphicx}
%\usepackage{pslatex}
%\usepackage[margin=2.25cm]{geometry}
\usepackage{multirow}

\usepackage{natbib}
\bibliographystyle{abbrvnat}

\newcommand{\degree}{\ensuremath{^\circ}}

\title{Literature Review of Point Cloud Compression}
\author{Keegan Smith\\ksmith@cs.uct.ac.za}

\begin{document}

\begin{titlepage}
\begin{center}

\includegraphics[width=100mm]{images/uct}\\
\ \\
\textsc{\Large
Department of Computer Science\\
\ \\
Honours Project Report\\
\ \\}

{\huge \bfseries
Intraframe Compression of Molecular Dynamics Simulations of Water
\\}
\ \\
\ \\

\begin{center}
    \large Keegan Carruthers-Smith
    \\
    \small{ksmith@cs.uct.ac.za}
\end{center}

\vfill

{\large \today}

\end{center}
\end{titlepage}


\section*{Abstract}
Molecular dynamics (MD) simulations generate vast amounts of data. A typical
100-million atom MD simulation produces approximately 5 gigabytes of data per
frame consisting of atom types, coordinates and velocities.

This report surveys techniques used in point cloud compression. A point cloud
is a collection of points in 3D space. It then investigates the structure of
water. We then present a method for compressing MD data using connectivity
based point cloud compression with predictors tailored towards the structure
of water. We compare this method to more generic point cloud and data
compressors.

\textbf{TODO} Talk about results etc once they have been produced!

\tableofcontents

\chapter{Introduction}

Molecular dynamics (MD) simulations generate vast amounts of data. A typical
100-million atom MD simulation produces approximately 5 gigabytes of data per
frame consisting of atom types, coordinates and velocities. This will generate
17 terabytes of data a day if run for $35\,000$ steps with every 10th frame
being saved \citep{omeltchenko2000sls}. Generating this much data makes good
compression very desirable.

We have developed a method which compresses MD simulations. It targets
simulations which have a large amount of water in them. It uses a connectivity
based point cloud technique, but with predictors which are based on models of
the structure of water.

\textbf{TODO} Flesh out introduction

\chapter{Background}

We will discuss common techniques used in point cloud compression algorithms,
and then move onto different categorizations and how they are applied. We will
then discuss how to apply these techniques to water molecule
compression. Finally, we will conclude with two general categorisations of
what the compression algorithms exploit.

\section{Point Cloud Compression}

Most research on point clouds involves point clouds of surfaces. This is
because most point clouds are obtained from 3D scans of object surfaces. A
surface is the ``skin'' of an object, i.e. the boundaries of a solid object.

\citet{gumholdcomp} create a tree of the points that exploits the knowledge
that the points lie on a surface. They do not, however, use predictors which
exploit this. \citet{merrycomp} extend on this by using predictors which do
exploit the rectilinear nature of surface scans.

\citet{omeltchenko2000sls} compress point clouds from molecular dynamics
simulations. To exploit the density of the point clouds, a space filling
curve\footnote{A space filling curve is one that touches every point in the
  space. \citet{omeltchenko2000sls} create this using a techniques similiar to
  an octree.} is created along the quantised positions. So, every point is
assigned an index along the curve. Then by encoding the difference between
successive indicies which contain points, they get a lot of common small
differences leading to good compression rates.

\citet{devillers2000gci} compress meshes of 3D models. The method they use
only considers point locations, so it can be used for point cloud
compression. Their method exploits a property of kd-trees, instead of general
geometric properties. A mesh is a collection of vertices, edges and faces
defining a 3D object.


\section{Quantisation}

Point cloud data usually stores the point coordinates in a floating point
format. Most geometric compression algorithms reduce the precision to achieve
better compression rates. This reduction of precision is called
\emph{quantisation} \citep{ag-racm-03}. Every algorithm reviewed in this paper
that uses quantisation, quantises the vertex positions for each coordinate
separately and uniformly in Cartesian space. This can be visualized as
creating an evenly-spaced grid over the dimensions of the space, and snapping
the points to the closest grid point. This is the most na\"{\i}ve method, but
there are more complex methods reviewed by \citet{ag-racm-03}. These lie
outside the scope of this survey. \citet{chen2005lcp} gets around quantisation
by treating the 32-bit floating point numbers as 32-bit integers.


\section{Predictive Compression}

Predictive compressors try to predict where a point is and then encode the
difference between its actual location and the predicted location (known as
the residual). These residuals are then encoded such that the decompressor can
calculate the point's location by the prediction added to the
residual. Entropy coding\footnote{Entropy encoding is a lossless compression
  scheme that does not exploit any characteristics of the underlying data.} is
usually applied to the encoding of residuals. If the chosen predictor(s)
exploit contextual information well, the residuals should have a low
entropy\footnote{A low entropy means we can predict what a residual will be
  well.}  leading to higher compression ratios.


\subsection{Predictors}

Both papers discussed depend on a spanning tree of the points. This will be
explained in the next subsection, but for now let $v'$ be the parent of the
point $v$, and $v''$ be the point's grandparent in the tree.

\citet{gumholdcomp} lets the user choose between two predictors. The first is
a \emph{constant} predictor where $v$ is predicted to be at $v'$. The second
is a \emph{linear} predictor where $v$ is predicted to be at $v' + (v' -
v'')$. So either $v$ is in the same place as its parent, or it is along the
straight line $(v', v'')$ with distance $|v'-v''|$ away from its parent.

\citet{merrycomp} use the same predictors as \citeauthor{gumholdcomp} but with
two additional predictors, the \emph{left} and \emph{right} predictors. The
surface normal\footnote{A surface normal at a point $v$ is the normal of the
  plane tangent to the surface at $v$.} $n_v$ at $v$ is heuristically
determined from $v, v'', v'$ and $n_{v'}$ (where $n_{v'}$ is the surface
normal at $v'$). From this they can determine what left and right are by using
the plane determined by $n_{v'}, v'$ and $v''$. Another difference to
\citeauthor{gumholdcomp} is that instead of letting the user choose the
predictor used, the best predictor is picked for each point. This requires the
predictor used at each point to be encoded, which is discussed next.


\subsection{Serialization}

Unlike meshes, point clouds have no connectivity information. So there is no
obvious order to serialize the vertices' information (generally residuals).

\citet{gumholdcomp} create a rooted spanning tree of the points. The points
are first sorted along the $x, y$ or $z$ axis. The tree is then greedily
created by picking the first point as a root and then adding each successive
point to the vertex which minimises the residual. The tree is then serialized
by entropy encoding the out degree of each vertex in breadth-first order.

\citet{merrycomp} also create a rooted spanning tree of the points, but in a
way that favours ``long runs'' of the forward predictor. They need to encode
the predictor used at each point, so encoding the tree as \citet{gumholdcomp}
did will not work. Instead the tree is serialized by entropy coding the
predictor used in depth-first order. By favouring long runs of the forward
predictor they get better compression ratios of the encoding of the tree. To
create the spanning tree, they first create a graph of the points where every
edge with length less than $L$ is added. $L$ is the length of the longest edge
in the minimum spanning tree of the complete graph of the points. The spanning
tree is then created similarly to Prim's algorithm \citep[p.\ 457]{sedgewick},
but instead using a metric\footnote{A metric is a way to calculate the
  distance between vertices.} that favours recently considered vertices and
ones predicted well by the forward predictor.

\citet{chen2005lcp} uses a spanning tree with differential coding on the
weights. So the point cloud is encoded as a tree, with edge weights being the
difference vectors between points. They show that there exists a spanning tree
which minimises the number of pairwise different edge weights, but also show
that this is NP-Hard. They do, however, give an approximate algorithm which
uses clustering and minimum spanning trees. The tree is serialized in a simple
breadth-first order, recording the number of children for each node and the
edge weights for each child.


\section{Decoding}

When decoding the data, we may present to the user intermediate
representations of the final decompressed data as we process it. Depending on
whether we can do this or not gives us another way to classify encoders.

\subsection{Progressive Encoding}

A progressive encoder is one that starts out streaming a coarse representation
of a point cloud. It then streams out refinements. This is useful for
transmitting the data over a network, as users can almost instantaneously see
a coarse representation. This is then followed by more and more refinements
over time as they arrive.

\citet{devillers2000gci} provide an example of a progressive coder. The method
is best explained in 1 dimension first. If you know the total number of points
in $[a, b]$ (say $x$ points) and the number of points in $[a, (a+b)/2]$ (say
$y$ points) then the number of points in $[(a+b)/2+1, b]$ is $x - y$ . By
sending out counts of deeper and deeper levels in breadth first order they
progressively get a better representation. Encoding this with arithmetic
coding gives us the desired compression ratios. To extend to 3 dimensions one
uses a process similiar to creating a kd-tree: at each step subdivide along a
different dimension.


\subsection{Single-Rate Encoding}

A single-rate encoder is one which requires the whole stream to be available
before the data can be decompressed. \citet{omeltchenko2000sls},
\citet{gumholdcomp} and \citet{merrycomp} are all single-rate encoders.


\section{Water Model}

A water dimer is two water molecules loosely bonded by a hydrogen atom. This
is the simplest model for hydrogen bonding in water, and as such has been
extensively studied.

The oxygen which is loosely bonded to the hydrogen is expected to be
$2.976\AA$ away from the other oxygen. It is also expected that the vector
created by the O-H bond to be roughly in line with the vector of the loose O-H
bond. See Figure \ref{fig:dimer} and Figure \ref{fig:dimr-angle}.

\textbf{TODO} references

\begin{figure}[h]
\centering
\resizebox{0.75\textwidth}{!}{
  \includegraphics{images/h402}
}
\caption{The most energetically favourable water dimer calculated from first
  principles.}
\label{fig:dimer}
\end{figure}

\begin{figure}[h]
\centering
\resizebox{0.5\textwidth}{!}{
  \includegraphics{images/dimr}
}
\caption{Water dimer angles. $R = 2.976\AA, \alpha = 6 \pm 20\degree, \beta =
  57 \pm 10\degree$}
\label{fig:dimr-angle}
\end{figure}

\subsection{Application of the Water Model in Point Cloud Compressors}

\citet{omeltchenko2000sls} method only exploits the fact that we have many
dense clusters in MD simulations. It is more general than water compression,
and modifying it to use the relative layouts of the water molecules is not
clear.

\citet{merrycomp} and \citet{gumholdcomp} predictors do not exploit the layout
of water molecules, but using predictors is a very salient idea. Using the
models we have about water molecules, we can predict where a water molecule
will occur relative to another one. The spanning tree would be different, but
similiar ideas will carry over.

Single-rate encoders usually have higher compression ratios than progressive
encoders. This survey covered progressive coders, but these are not as
applicable to MD compression since the application of the MD data requires the
whole file. So we will not discuss them further.


\section{Summary}

Refer to Figure \ref{fig:taxonomy} for an overview of the categorisations of
the different papers. There are two underlying philosophies which the
compression algorithms exploit, the \emph{scope} and the \emph{topology}.

The \emph{scope} is whether the algorithm is exploiting \emph{global} or
\emph{local} properties of the points. citet{merrycomp} uses a local scope
since it exploits nearby points to predict the location of the current
point. \citet{chen2005lcp} uses a global scope, since it is trying to minimise
a global property of the spanning tree.

The \emph{topology} is whether the algorithm is trying to exploit connectivity
or geometric information. \citet{gumholdcomp} exploits both, but the geometry
slightly more since it uses a predictor to predict the location based on other
points location. \citet{chen2005lcp} just tries to choose the best possible
spanning tree, so they exploit connectivity of points.

For the compression of water molecules we have local heuristics. From these we
have good geometric and connectivity information. So the best matching
approach would come from something similiar to the predictive single-rate
compression algorithms of \citet{merrycomp} and \citet{gumholdcomp}.

\begin{figure*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{||l|c|c||}
  \hline
  \emph{Attribute/Method} & \citet{omeltchenko2000sls} & \citet{gumholdcomp} \\

  \hline

  \emph{Structure} & MD Simulation & Surface \\

  \emph{Progressive} & Single-rate & Single-rate \\

  \emph{Quantisation} & Yes & Yes \\

  \emph{Connectivity} & Space Filling Curve & Spanning Tree \\

  \emph{Exploits} & Dense/regular clusters & Regularity of scans \\

  \emph{Scope} & Global & Local \\

  \emph{Topology} & Geometry & Both \\

  \hline
  \hline

  \emph{Attribute/Method} & \citet{chen2005lcp} & \citet{devillers2000gci} \\

  \hline

  \emph{Structure} & 3D Model & Mesh \\

  \emph{Progressive} & Single-rate & Progressive \\

  \emph{Quantisation} & No & Yes \\

  \emph{Connectivity} & Spanning Tree & kd-trees \\

  \emph{Exploits} & Differential Coding & kd-tree properties \\

  \emph{Scope} & Global & Global \\

  \emph{Topology} & Connectivity & Neither \\

  \hline
  \hline

  \emph{Attribute/Method} & \citet{merrycomp} & \\

  \hline

  \emph{Structure} & Surface & \\

  \emph{Progressive} & Single-rate & \\

  \emph{Quantisation} & Yes & \\

  \emph{Connectivity} & Spanning Tree & \\

  \emph{Exploits} & Rectilinear scans & \\

  \emph{Scope} & Local & \\

  \emph{Topology} & Both & \\

  \hline
\end{tabular}
}
\caption{Taxonomy of the different methods.}\label{fig:taxonomy}
\end{figure*}


\chapter{Design}

This section outlines the design for our compressor and decompressor. The
compression techniques I investigate only cover intra-frame
compression\footnote{intra-frame compression is where we compress each frame
  independently of the other frames.}, but our final product handles
multiframe compression and visualisation as well. As such the design needs to
take into account these uses, but I will only cover the design aspects
relevant to single frame compression.

I worked closely with Julian Kenwood on the compressor and decompressor, and
areas designated to him are more thoroughly explained in his report. I will
indicate such areas.

\section{Overview}

We target the PDB format, which is a format for storing MD simulations. PDB is
readable by VMD. \textbf{TODO} improve and references.

A high-level overview of how our compressors work is illustrated by the
following: \textbf{TODO} replace with pretty picture

\[ DCDFile \to Atoms \to QuantisedAtoms \to Compressor \]

The decompressor works in the reverse direction

\[ Decompressor \to QuantisedAtoms \to DCDFile \]

The decompressed DCD files have been quantised. Excluding the quantisation
step, our scheme is lossless. Notice that each step in the compression phase
has auxillary information. The auxillary information is required to
reconstruct reverse the process.


\section{Components}

We took a component-based approach to the design of the compressor. This is
because our algorithm naturally breaks down into different components. Our
reference implementations of other algorithms also make use of these
components.

The different components are the following:
\begin{itemize}
\item Compressors
\item Simulation I/O
\item Core Transformations
\item Arithmetic Coders
\item Miscellaneous Components
\item Verification Components
\end{itemize}

\subsection{Compressors}

The compressor component is the driver program. It utilises the other
components listed below to do compression and decompression. There is our
compressor and three reference compressors. The reference compressors are the
\citet{omeltchenko2000sls} scheme, \cite{gumholdcomp} scheme and the
\citet{devillers2000gci} scheme.


\subsection{Simulation I/O}

These components handle I/O to and from the files we are compressing. We are
compressing DCD files. DCD files come with accompanying PDB file, which
contains info about what each atom is in the DCD file.

\subsection{Core Transformation}

These components implement the core algorithms for each different
compressor. There are components for each reference implementation and our own
compressor.

\textbf{TODO} do this in a section about our compressor

\begin{verbatim}
Transform(Atoms *atoms, ArithmeticEncoder output):
  Atoms *water_atoms, *other_atoms = FindWater(atoms)
  Graph water_graph = CreateGraph(water_atoms)
  Tree spanning_tree = CreateSpanningTree(water_graph)
  TreeSerialise(output, tree)
\end{verbatim}

\textbf{TODO} detail the steps

\section{Arithmetic Coder}

The ``meat'' of all the compression schemes convert the input into a symbol
stream (where the symbols are from a fixed alphabet). These symbols are then
compressed with an arithmetic encoder. Similarly the decompressor takes a
symbol stream and converts it into the original input.

The arithmetic coder has two parts to it, a \emph{reader} and a
\emph{writer}. The \emph{reader} and the \emph{writer} are inverses to each
other, with the \emph{reader} converting a symbol stream to a bit stream and
the \emph{writer} converting a bit stream to a symbol stream.

The scheme by \citet{omeltchenko2000sls} does not make use of an Arithmetic
Coder, instead it uses its own coder. The implementation of the coders is
handled by Julian.


\section{Random sections to be integrated properly}

\subsection{Compressing Permutations}

Every compression algorithm explored in this report does not necessarily
preserve the order of the points when decompressed. For example in
\cite{devillers2000gci}
\[ (1, 2), (1, 3), (0, 0), (2, 3), (2, 2), (0, 2), (1, 1) \]
when decompressed becomes
\[ (0, 0), (1, 1), (0, 2), (1, 2), (1, 3), (2, 2), (2, 3) \]

When decompressing we need to recover the original order the points, since the
index of a point indicates which atom it is in the DCD file format.

We experimented with five different types of permutation compressors. Each
permutation compressor takes in a permutation of $[0,1,\dots,N-1]$ and writes
it to the file. Let the permutation be $[P_0, P_1, \dots P_{N-1}]$.  The
decompressor then recovers the permutation.

\paragraph{Null} does not write anything to the file. The decompressor just 
outputs the order permutation $[0,1,\dots,N-1]$. A compressor which uses the
null compressor does not preserve the order. Every permutation compressor
ratios are compared to this method, since \textbf{TODO} somehow explain why we
use this and why it is used to compare against other compressors.

\paragraph{Na\"{\i}ve} writes the permutation list straight to the file. Each index
is stored as a $32$-bit integer.

\paragraph{Delta} transforms the permutation into a list $L$, where $L_0 =
P_0$ and $L_i = P_i - P_{i-1}$ for $0 < i < N$. We then use arithmetic
encoding on $L$ and write that to the file. When we decompress we get the list
$L$. To get the permutation back we use the fact that $P_0 = L_0$ and $P_i =
L_i + P_{i-1}$ for $0 < i < N$. If we calculate $P_{i-1}$ before $P_{i}$, the
we can successfully recover $P_{i}$.

\paragraph{Interframe} transforms the permutation $P$ into a list $L$. Let
$P'$ be the permutation from the previous frame. If it is the first frame, let
$P' = [0,1,\dots,N-1]$. Then $L_i = P_i - P'_i$ for $0 \le i < N$. We then use
arithmetic encoding on $L$ and write that to the file. When we decompress we
get the list $L$. To get the permutation back we use the fact that $P_i = L_i
+ P'_i$. Note that this method requires us to decompress the frames in order.

\paragraph{Optimal} uses a static arithmetic coder with a frequency of $1$ for
each $P_i$. Since each $P_i$ occurs only once, this gives the best possible
compression.

A static arithmetic encoder model takes roughly $O(\log(N))$ to update the
symbol table for each encoding. In the optimal permutation compressor we can
get this down to $O(1)$ by using the fact that we only ever encode each $P_i$
once. To encode a symbol $i$ the arithmetic encoder requires $S_{tot}$, $S_i$
and $S_{i-1}$. $S_{tot}$ is the sum of all the frequencies. $S_i$ is the sum
of the frequencies for all the symbols less than or equal to
$i$. \textbf{TODO} explain algorithm

\subsubsection{Water Predictors}

There are three predictors used, the constant predictor and the two hydrogen
predictors. Let $O, H_1, H_2$ be the predictions for where the water molecule
will be and let $O', H'_1, H'_2$ be the water molecules parent.

\paragraph{Constant Predictor} predicts $O, H_1, H_2$ to be at $O'$.

\paragraph{Hydrogen Predictor} predicts along $O'-H'_1$ or $O'-H'_2$. We
simplify the model in Figure \ref{fig:dimer-angle} and assume $\alpha$ is
$0\degree$. We predict $O$ to be at $O' + 2.976\frac{O'-H'_i}{|O'-H'_i|}$. Due
to the large variability of $\beta$ in Figure \ref{fig:dimer-angle} we predict
$H_1$ and $H_2$ to be at $O$ in both predictors.


\nocite{*}
\bibliography{report_keegan}

\end{document}
